{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Luminosity dependency of reconstruction error on split 10/90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import h5py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make ROC_curve\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score                          \n",
    "\n",
    "def get_roc_curve(label, scores, names):\n",
    "    \"\"\"Generates ROC Curves for a given array\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        fpr, tpr, thresholds = roc_curve(label, scores[i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr,\n",
    "                 tpr,\n",
    "                 linewidth=3,\n",
    "                 #linestyle=line_styles[0],\n",
    "                 label=(\"%s AUC: %s\" % (names[i], roc_auc)))\n",
    "        \n",
    "    plt.legend(frameon=False)\n",
    "    plt.ylabel(\"Sensitivity (TPR)\")\n",
    "    plt.xlabel(\"Fall-out (TNR)\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get permission to access EOS (Insert your NICE password)\n",
    "\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_dir_01 = \"/eos/user/t/tkrzyzek/autoencoder/lumi_dep/split01drop/\"\n",
    "files_dir_08 = \"/eos/user/t/tkrzyzek/autoencoder/lumi_dep/split08/\"\n",
    "\n",
    "files_dir = files_dir_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_pred = pickle.load(open(files_dir + \"ae_pred.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms_scores = pickle.load(open(files_dir + \"ms_scores.p\", \"rb\"))\n",
    "rf_scores = pickle.load(open(files_dir + \"rf_scores.p\", \"rb\"))\n",
    "true_labels = pickle.load(open(files_dir + \"true_labels.p\", \"rb\"))\n",
    "luminosity = pickle.load(open(files_dir + \"luminosity.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pickle.load(open(files_dir + \"x_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels = true_labels[0]\n",
    "ms_scores = ms_scores[0]\n",
    "rf_scores = rf_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error_df(X_test, predictions, mode=\"allmean\", n_highest = 100):\n",
    "    \n",
    "    if mode == \"allmean\":\n",
    "        return np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "    \n",
    "    elif mode == \"topn\":\n",
    "        temp = np.partition(-np.power(X_test - predictions, 2), n_highest)\n",
    "        result = -temp[:,:n_highest]\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    elif mode == \"perobj\":\n",
    "        mses = []\n",
    "        for l in legend:\n",
    "            mse = np.mean(\n",
    "                np.power(X_test[:,l[\"start\"]:l[\"end\"]] - predictions[:,l[\"start\"]:l[\"end\"]], 2),\n",
    "                axis=1)\n",
    "            mses.append(mse)\n",
    "     \n",
    "        return np.maximum.reduce(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_scores = get_error_df(X_test, ae_pred, mode=\"topn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"Autoencoder\", \"Mean square\", \"Random forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inliers = len(X_test[true_labels == 0])\n",
    "outliers = len(X_test[true_labels == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Good lumis:\", inliers)\n",
    "print(\"Bad lumis:\", outliers)\n",
    "print(\"Ratio of bad lumis:\", outliers/len(true_labels))\n",
    "get_roc_curve(true_labels, [ae_scores, ms_scores, rf_scores], names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index from zero\n",
    "luminosity = luminosity.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_lumis():\n",
    "    lumi_df = pd.DataFrame({'luminosity': luminosity,\n",
    "                           'true_label': true_labels})\n",
    "    \n",
    "    print(lumi_df[start:end].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_vs_feature(y_val, x_val, y_class, y_name=\"\", x_name=\"\", selected=[], linear=False,\n",
    "                    limit=False):\n",
    "    '''\n",
    "    Plots two arbitrary values agains each other.\n",
    "    '''\n",
    "    df = pd.DataFrame({'y_val': y_val,\n",
    "                       'x_val': x_val,\n",
    "                       'y_class': y_class})\n",
    "\n",
    "    groups = df.groupby('y_class')\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x_val, \n",
    "                group.y_val,\n",
    "                color=\"red\" if name == 1 else \"blue\",\n",
    "                marker='o',\n",
    "                ms=2,\n",
    "                linestyle='',\n",
    "                label= \"Anomaly\" if name == 1 else \"Normal\")\n",
    "\n",
    "    for i in selected:\n",
    "        ax.plot(x_val[i],\n",
    "                y_val[i],\n",
    "                color=\"green\",\n",
    "                marker='o',\n",
    "                ms=4,\n",
    "                linestyle='')\n",
    "                 \n",
    "    ax.legend()\n",
    "    if not linear:\n",
    "        ax.set_yscale('log')\n",
    "    if limit:\n",
    "        plt.xlim([limit[0], limit[1]])\n",
    "    plt.grid()\n",
    "    plt.ylabel(y_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luminosity across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(luminosity, range(len(true_labels)), true_labels, \"Luminosity\", \n",
    "                \"Lumisection #\", linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction error across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores, range(len(true_labels)), true_labels, \"AE reco error\", \n",
    "                \"Lumisection #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores_no_reg, range(len(true_labels)), true_labels, \"AE reco error\", \n",
    "                \"Lumisection #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to select lumisections range of the bad lumis cluster. Selected lumisections are marked in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "begin = 0\n",
    "end = 0\n",
    "\n",
    "lumis_range = range(begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores, range(len(true_labels)), true_labels, \"AE reco error\", \n",
    "                \"Lumisection #\", selected=lumis_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected lumisections' luminosity (again in green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(luminosity, range(len(true_labels)), true_labels, \"Luminosity\", \n",
    "                \"Lumisection #\", linear=True, selected=lumis_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom in to the selected range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(luminosity, range(len(true_labels)), true_labels, \"Luminosity\", \n",
    "                \"Lumisection #\", linear=True, selected=lumis_range, limit=(begin, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of luminosity of the selected lumisections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(luminosity[begin:end], bins=np.arange(0, 0.4, 0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction error vs luminosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE reconstruction error vs luminosity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ms_scores, luminosity, true_labels, \"AE reco error\", \"Luminosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores_no_reg, luminosity, true_labels, \"AE w/o regularization reco error\", \"Luminosity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores, luminosity, true_labels, \"Mean square\", \"Luminosity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... with the cluster selected above marked in green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_vs_feature(ae_scores, luminosity, true_labels, \"AE reco error\", \"Luminosity\",\n",
    "               lumis_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STEP = 0.05\n",
    "#SAMPLE_SIZE = 2377\n",
    "SAMPLE_SIZE=100\n",
    "\n",
    "features_good = []\n",
    "features_bad = []\n",
    "\n",
    "good_mean_mean = []\n",
    "good_mean_median = []\n",
    "bad_mean_mean = []\n",
    "bad_mean_median = []\n",
    "\n",
    "ae_auc_scores = []\n",
    "ms_auc_scores = []\n",
    "\n",
    "def plot_mean_features(X, y):\n",
    "    \n",
    "    lower_bounds = np.arange(0, 0.35, STEP)\n",
    "    X_orig = X\n",
    "    y_orig = y\n",
    "    \n",
    "    for i in lower_bounds:\n",
    "        print(\"Luminosity range:\", i, \"-\", i+STEP)\n",
    "        \n",
    "        X = pd.DataFrame(X_orig)\n",
    "        \n",
    "        X = X[(luminosity >= i) & (luminosity < i+STEP)]\n",
    "        y = y_orig[(luminosity >= i) & (luminosity < i+STEP)]\n",
    "        print(\"Dataset size:\", len(X))\n",
    "        \n",
    "        X = X.iloc[:, 0:2806]\n",
    "        X_good = X[y == 0]\n",
    "        X_bad = X[y == 1]\n",
    "\n",
    "        print(\"Good samples:\", len(X_good))\n",
    "        print(\"Bad samples:\", len(X_bad))\n",
    "            \n",
    "        sample_good = X_good.sample(SAMPLE_SIZE)        \n",
    "        good_mean = abs(np.mean(sample_good, axis=0))\n",
    "        good_median = abs(np.median(sample_good, axis=0))\n",
    "        \n",
    "        if len(X_bad) > 0:\n",
    "            ae_auc = roc_auc_score(y, ae_scores[(luminosity >= i) & (luminosity < i+STEP)])\n",
    "            ae_auc_scores.append(ae_auc)\n",
    "            print(\"AE ROC AUC:\", ae_auc)\n",
    "\n",
    "            ms_auc = roc_auc_score(y, ms_scores[(luminosity >= i) & (luminosity < i+STEP)])\n",
    "            ms_auc_scores.append(ms_auc)\n",
    "            print(\"MS ROC AUC:\", ms_auc)\n",
    "        \n",
    "            bad_sample = X_bad.sample(SAMPLE_SIZE)\n",
    "            bad_mean = abs(np.mean(bad_sample, axis=0))\n",
    "            bad_median = abs(np.median(bad_sample, axis=0))\n",
    "            bad_mean_mean.append(np.mean(bad_mean))\n",
    "            bad_mean_median.append(np.mean(bad_median))\n",
    "        else:\n",
    "            bad_mean_mean.append(0)\n",
    "            bad_mean_median.append(0)\n",
    "            \n",
    "        good_mean_mean.append(np.mean(good_mean))\n",
    "        good_mean_median.append(np.mean(good_median))\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "        \n",
    "        ax1.plot(np.array([i for i in range(len(bad_mean))]),\n",
    "                 bad_mean,\n",
    "                 color=\"red\",\n",
    "                 linestyle='-',\n",
    "                 ms=2,\n",
    "                 marker='o',\n",
    "                 alpha=0.5,\n",
    "                 label=\"Bad\")\n",
    "                 \n",
    "        ax1.plot(np.array([i for i in range(len(good_mean))]),\n",
    "                 good_mean,\n",
    "                 color=\"b\",\n",
    "                 linestyle='-',\n",
    "                 ms=2,\n",
    "                 marker='o',\n",
    "                 alpha=0.5,\n",
    "                 label=\"Good\")\n",
    "        \n",
    "        ax2.plot(np.array([i for i in range(len(bad_median))]),\n",
    "                 bad_median,\n",
    "                 color=\"red\",\n",
    "                 linestyle='-',\n",
    "                 ms=2,\n",
    "                 alpha=0.5,\n",
    "                 marker='o',\n",
    "                 label=\"Bad\")\n",
    "                 \n",
    "        ax2.plot(np.array([i for i in range(len(good_median))]),\n",
    "                 good_median,\n",
    "                 color=\"b\",\n",
    "                 linestyle='-',\n",
    "                 alpha=0.5,\n",
    "                 ms=2,\n",
    "                 marker='o',\n",
    "                 label=\"Good\")\n",
    "        \n",
    "        ax1.legend()\n",
    "        ax2.legend()\n",
    "        ax1.set_ylim(0, 100)\n",
    "        ax2.set_ylim(0, 100)\n",
    "        ax1.set_title(\"Mean\")\n",
    "        ax2.set_title(\"Median\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots showing mean and median of each feature across randomly chosen 100 good and 100 bad lumisections (separately) in luminosity ranges\n",
    "\n",
    "Note: values of test set scaled with parameters from training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plot_mean_features(X_test, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_means():\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    ax1.bar(np.arange(0, 0.35, STEP),\n",
    "           good_mean_mean,\n",
    "           align='edge',\n",
    "           width=0.05,\n",
    "           color='b',\n",
    "           alpha=0.5,\n",
    "           label=\"Good\")\n",
    "\n",
    "    ax1.bar(np.arange(0, 0.35, STEP),\n",
    "           bad_mean_mean,\n",
    "           align='edge',\n",
    "           width=0.05,\n",
    "           color='r',\n",
    "           alpha=0.5,\n",
    "           label=\"Bad\")\n",
    "\n",
    "    ax1.set_ylabel(\"Mean mean\")\n",
    "    ax1.set_ylim((0, 5))\n",
    "    ax1.set_title('Mean mean of lumisections')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.bar(np.arange(0, 0.35, STEP),\n",
    "           good_mean_median,\n",
    "           align='edge',\n",
    "           width=0.05,\n",
    "           color='b',\n",
    "           alpha=0.5,\n",
    "           label=\"Good\")\n",
    "\n",
    "    ax2.bar(np.arange(0, 0.35, STEP),\n",
    "           bad_mean_median,\n",
    "           align='edge',\n",
    "           width=0.05,\n",
    "           color='r',\n",
    "           alpha=0.5,\n",
    "           label=\"Bad\")\n",
    "\n",
    "    ax2.set_ylim((0, 5))\n",
    "    ax2.set_ylabel(\"Mean median\")\n",
    "    ax2.set_title('Mean median of lumisections')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.xlabel('Luminosity')\n",
    "    ax2.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean of above values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_ae_vs_ms():\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.bar(np.arange(0, 0.3, STEP),\n",
    "           ae_auc_scores,\n",
    "           align='edge',\n",
    "           width=0.02,\n",
    "           color='b',\n",
    "           alpha=0.5,\n",
    "           label=\"Autoencoder\")\n",
    "\n",
    "    ax1.bar(np.arange(0.02, 0.32, STEP),\n",
    "           ms_auc_scores,\n",
    "           align='edge',\n",
    "           width=0.02,\n",
    "           color='r',\n",
    "           alpha=0.5,\n",
    "           label=\"Mean square\")\n",
    "\n",
    "    ax1.set_ylabel(\"ROC AUC\")\n",
    "    ax1.set_ylim((0.4, 1))\n",
    "    ax1.set_xlim((0, 0.35))\n",
    "    ax1.set_title('AE vs MS')\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Luminosity')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder vs mean square in luminosity ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "plot_ae_vs_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
