{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running on GPU?\n",
    "import setGPU\n",
    "\n",
    "import getpass\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get permission to access EOS (Insert your NICE password)\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())\n",
    "\n",
    "# ## Load data, and labels\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CUTOFF_ACC = 1000\n",
    "# CUTOFF_ACC = 10\n",
    "\n",
    "PDs  = {1: 'BTagCSV',\n",
    "        2: 'BTagMu',\n",
    "        3: 'Charmonium',\n",
    "        4: 'DisplacedJet',\n",
    "        5: 'DoubleEG',\n",
    "        6: 'DoubleMuon',\n",
    "        7: 'DoubleMuonLowMass',\n",
    "        8: 'FSQJets',\n",
    "        9: 'HighMultiplicityEOF',\n",
    "        10: 'HTMHT',\n",
    "        11: 'JetHT',\n",
    "        12: 'MET',\n",
    "        13: 'MinimumBias',\n",
    "        14: 'MuonEG',\n",
    "        15: 'MuOnia',\n",
    "        16: 'NoBPTX',\n",
    "        17: 'SingleElectron',\n",
    "        18: 'SingleMuon',\n",
    "        19: 'SinglePhoton',\n",
    "        20: 'Tau',\n",
    "        21: 'ZeroBias'}\n",
    "\n",
    "# Select PD\n",
    "nPD = 18\n",
    "\n",
    "data_directory = \"/eos/cms/store/user/fsiroky/consistentlumih5/\"\n",
    "label_file = \"/afs/cern.ch/user/t/tkrzyzek/Documents/Data-Certification/JetHT.json\"\n",
    "model_directory = \"/eos/user/t/tkrzyzek/autoencoder/single_muon/\"\n",
    "model_name = \"model\"\n",
    "\n",
    "def get_file_list(directory, pds, npd, typeof, extension):\n",
    "    files = []\n",
    "    parts = [\"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    for p in parts:\n",
    "        files.append(\"%s%s_%s_%s%s\" % (directory, pds[npd], p, typeof, extension))\n",
    "    return files\n",
    "\n",
    "files = get_file_list(data_directory, PDs, nPD, \"background\", \".h5\")\n",
    "files = files + get_file_list(data_directory, PDs, nPD, \"signal\", \".h5\")\n",
    "\n",
    "# Load good and bad jets\n",
    "def get_data(files):\n",
    "    readout = np.empty([0,2813])\n",
    "    \n",
    "    for file in files:\n",
    "        jet = file.split(\"/\")[-1][:-3]\n",
    "        print(\"Reading: %s\" % jet)\n",
    "        try:\n",
    "            h5file = h5py.File(file, \"r\")\n",
    "            readout = np.concatenate((readout, h5file[jet][:]), axis=0)\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have %s. %s\" % (jet, error))\n",
    "            continue\n",
    "\n",
    "    return readout\n",
    "\n",
    "def get_error_df(X_test, predictions, mode=\"allmean\", n_highest = 100):\n",
    "    \n",
    "    if mode == \"allmean\":\n",
    "        return np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "    \n",
    "    elif mode == \"topn\":\n",
    "        temp = np.partition(-np.power(X_test - predictions, 2), n_highest)\n",
    "        result = -temp[:,:n_highest]\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    elif mode == \"perobj\":\n",
    "        mses = []\n",
    "        for l in legend:\n",
    "            mse = np.mean(\n",
    "                np.power(X_test[:,l[\"start\"]:l[\"end\"]] - predictions[:,l[\"start\"]:l[\"end\"]], 2),\n",
    "                axis=1)\n",
    "            mses.append(mse)\n",
    "     \n",
    "        return np.maximum.reduce(mses)\n",
    "    \n",
    "def find_optimal_cutoff(scores, y_true):\n",
    "    step_factor = CUTOFF_ACC\n",
    "    max_acc = 0\n",
    "    best_threshold = None\n",
    "    for threshold in tqdm(np.geomspace(min(scores), max(scores), step_factor)):\n",
    "        y_pred = [1 if e > threshold else 0 for e in scores]\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, max_acc\n",
    "\n",
    "data = pd.DataFrame(get_data(files))\n",
    "\n",
    "data[\"run\"] = data[2807].astype(int)\n",
    "data[\"lumi\"] = data[2808].astype(int)\n",
    "data[\"inst_lumi\"] = data[2809].astype(float)\n",
    "\n",
    "# Drop unnecessary meta data\n",
    "data.drop([2807, 2808, 2809, 2810, 2811, 2812], axis=1, inplace=True)\n",
    "\n",
    "# Sort by runID and then by lumiID\n",
    "data = data.sort_values([\"run\", \"lumi\"], ascending=[True,True])\n",
    "\n",
    "# Reset index\n",
    "data = data.reset_index(drop=True)  \n",
    "\n",
    "runIDs  = data[\"run\"].astype(int)\n",
    "lumiIDs = data[\"lumi\"].astype(int)\n",
    "luminosity = data[\"inst_lumi\"].astype(float)\n",
    "\n",
    "# Apply labels\n",
    "output_json = json.load(open(label_file))\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    try:\n",
    "        for i in json_file[str(int(orig_runid))]:\n",
    "            if orig_lumid >= i[0] and orig_lumid <= i[1]:\n",
    "                return 0\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return 1\n",
    "\n",
    "def add_flags(sample):\n",
    "    return json_checker(output_json, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "data[\"label\"] = data.apply(add_flags, axis=1)\n",
    "\n",
    "# Split the data\n",
    "PRE_TRAIN = 0.1\n",
    "\n",
    "split = round(PRE_TRAIN*len(data))\n",
    "\n",
    "runIDs = runIDs[split:]\n",
    "lumiIDs = lumiIDs[split:]\n",
    "luminosity = luminosity[split:]\n",
    "\n",
    "train = data.iloc[:split]\n",
    "X_train = train.iloc[:, 0:2806]\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "test = data.iloc[split:]\n",
    "X_test = test.iloc[:, 0:2806]\n",
    "y_test = np.asarray(test[\"label\"])\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "inliers = []\n",
    "outliers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2806)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              5614000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 500)               500       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000)              2002000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2806)              5614806   \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 2806)              2806      \n",
      "=================================================================\n",
      "Total params: 16,242,612\n",
      "Trainable params: 16,242,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# DEFINE MODELS\n",
    "######################\n",
    "# Calculate class weights\n",
    "#classes = np.unique(y_train)\n",
    "#weights = class_weight.compute_class_weight('balanced', classes, y_train)\n",
    "#cw = {int(cls): weight for cls, weight in zip(classes, weights)}\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 7,\n",
    "    \"n_estimators\": 64, \n",
    "    \"random_state\": 42, \n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\" : 1,\n",
    "#    \"class_weight\": cw\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**params)\n",
    "\n",
    "# Train only on good\n",
    "X_train_good = X_train[y_train == 0]\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(input_layer)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(500, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "decoder = Dense(input_dim)(encoded)\n",
    "decoder = prellll(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "adamm = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_name)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")\n",
    "\n",
    "ae_scores = []\n",
    "ae_cutoffs = []\n",
    "ms_scores = []\n",
    "rf_scores = []\n",
    "true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11219 samples, validate on 3740 samples\n",
      "Epoch 1/2048\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# PRE-TRAIN\n",
    "######################\n",
    "history = autoencoder.fit(X_train_good,\n",
    "                          X_train_good,\n",
    "                          epochs=2048,\n",
    "                          batch_size=256,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.25,\n",
    "                          verbose=2,\n",
    "                          callbacks=[early_stopper, checkpoint_callback]).history\n",
    "\n",
    "ae_pred = autoencoder.predict(X_train)\n",
    "ae_score = get_error_df(X_train, ae_pred, mode=\"topn\")\n",
    "ae_cutoff, ae_acc = find_optimal_cutoff(ae_score, y_train)\n",
    "ae_base_cutoff = ae_cutoff\n",
    "\n",
    "# Mean square\n",
    "mean_square = np.mean(np.power(X_test, 2), axis=1)\n",
    "\n",
    "# Random forest\n",
    "rf.fit(X_train, y_train)\n",
    "X_rf = X_train\n",
    "y_rf = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
