{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running on GPU?\n",
    "import setGPU\n",
    "\n",
    "import getpass\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "# Get permission to access EOS (Insert your NICE password)\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense, Lambda, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.activations import sigmoid, linear, relu\n",
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose dataset (if you change it, be sure to also change the label file below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PDs  = {1: 'BTagCSV',\n",
    "        2: 'BTagMu',\n",
    "        3: 'Charmonium',\n",
    "        4: 'DisplacedJet',\n",
    "        5: 'DoubleEG',\n",
    "        6: 'DoubleMuon',\n",
    "        7: 'DoubleMuonLowMass',\n",
    "        8: 'FSQJets',\n",
    "        9: 'HighMultiplicityEOF',\n",
    "        10: 'HTMHT',\n",
    "        11: 'JetHT',\n",
    "        12: 'MET',\n",
    "        13: 'MinimumBias',\n",
    "        14: 'MuonEG',\n",
    "        15: 'MuOnia',\n",
    "        16: 'NoBPTX',\n",
    "        17: 'SingleElectron',\n",
    "        18: 'SingleMuon',\n",
    "        19: 'SinglePhoton',\n",
    "        20: 'Tau',\n",
    "        21: 'ZeroBias'}\n",
    "\n",
    "# Select PD\n",
    "nPD = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legend = [{\"name\": 'pf_jets', \"start\": 0, \"end\": 776},\n",
    "          {\"name\": 'cal_jet_mets', \"start\": 777, \"end\": 944},\n",
    "          {\"name\": 'pho', \"start\": 945, \"end\": 1280},\n",
    "          {\"name\": 'muons', \"start\": 1281, \"end\": 1784},\n",
    "          {\"name\": 'pf_jets2', \"start\": 1785, \"end\": 1889},\n",
    "          {\"name\": 'pf_mets', \"start\": 1890, \"end\": 1917},\n",
    "          {\"name\": 'nvtx', \"start\": 1918, \"end\": 1924},\n",
    "          {\"name\": 'cal_jet_mets2', \"start\": 1925},\n",
    "          {\"name\": 'sc', \"start\": 2037, \"end\": 2127},\n",
    "          {\"name\": 'cc', \"start\": 2128, \"end\": 2169},\n",
    "          {\"name\": 'pho2', \"start\": 2170, \"end\": 2365},\n",
    "          {\"name\": 'muons2', \"start\": 2366, \"end\": 2491},\n",
    "          {\"name\": 'ebs', \"start\": 2492, \"end\": 2701},\n",
    "          {\"name\": 'hbhef', \"start\": 2702, \"end\": 2764},\n",
    "          {\"name\": 'presh', \"start\": 2765, \"end\": 2806},\n",
    "          {\"name\": 'inst_lumi', \"start\": 2807, \"end\": 2808},\n",
    "          {\"name\": 'pileup', \"start\": 2808, \"end\": 2809}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable names (7 features for each variable, except for the last 2, which are inst. luminosity and pileup):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature description\n",
    "feature_names = ['qPFJetPt', 'qPFJetEta', 'qPFJetPhi', 'qPFJet0Pt', 'qPFJet1Pt', 'qPFJet2Pt', 'qPFJet3Pt', 'qPFJet4Pt', 'qPFJet5Pt', 'qPFJet0Eta', 'qPFJet1Eta', 'qPFJet2Eta', 'qPFJet3Eta', 'qPFJet4Eta', 'qPFJet5Eta', 'qPFJet0Phi', 'qPFJet1Phi', 'qPFJet2Phi', 'qPFJet3Phi', 'qPFJet4Phi', 'qPFJet5Phi', 'qPFJet4CHS0Pt', 'qPFJet4CHS1Pt', 'qPFJet4CHS2Pt', 'qPFJet4CHS3Pt', 'qPFJet4CHS4Pt', 'qPFJet4CHS5Pt', 'qPFJet4CHS0Eta', 'qPFJet4CHS1Eta', 'qPFJet4CHS2Eta', 'qPFJet4CHS3Eta', 'qPFJet4CHS4Eta', 'qPFJet4CHS5Eta', 'qPFJet4CHS0Phi', 'qPFJet4CHS1Phi', 'qPFJet4CHS2Phi', 'qPFJet4CHS3Phi', 'qPFJet4CHS4Phi', 'qPFJet4CHS5Phi', 'qPFJet8CHS0Pt', 'qPFJet8CHS1Pt', 'qPFJet8CHS2Pt', 'qPFJet8CHS3Pt', 'qPFJet8CHS4Pt', 'qPFJet8CHS5Pt', 'qPFJet8CHS0Eta', 'qPFJet8CHS1Eta', 'qPFJet8CHS2Eta', 'qPFJet8CHS3Eta', 'qPFJet8CHS4Eta', 'qPFJet8CHS5Eta', 'qPFJet8CHS0Phi', 'qPFJet8CHS1Phi', 'qPFJet8CHS2Phi', 'qPFJet8CHS3Phi', 'qPFJet8CHS4Phi', 'qPFJet8CHS5Phi', 'qPFJetEI0Pt', 'qPFJetEI1Pt', 'qPFJetEI2Pt', 'qPFJetEI3Pt', 'qPFJetEI4Pt', 'qPFJetEI5Pt', 'qPFJetEI0Eta', 'qPFJetEI1Eta', 'qPFJetEI2Eta', 'qPFJetEI3Eta', 'qPFJetEI4Eta', 'qPFJetEI5Eta', 'qPFJetEI0Phi', 'qPFJetEI1Phi', 'qPFJetEI2Phi', 'qPFJetEI3Phi', 'qPFJetEI4Phi', 'qPFJetEI5Phi', 'qPFJet8CHSSD0Pt', 'qPFJet8CHSSD1Pt', 'qPFJet8CHSSD2Pt', 'qPFJet8CHSSD3Pt', 'qPFJet8CHSSD4Pt', 'qPFJet8CHSSD5Pt', 'qPFJet8CHSSD0Eta', 'qPFJet8CHSSD1Eta', 'qPFJet8CHSSD2Eta', 'qPFJet8CHSSD3Eta', 'qPFJet8CHSSD4Eta', 'qPFJet8CHSSD5Eta', 'qPFJet8CHSSD0Phi', 'qPFJet8CHSSD1Phi', 'qPFJet8CHSSD2Phi', 'qPFJet8CHSSD3Phi', 'qPFJet8CHSSD4Phi', 'qPFJet8CHSSD5Phi', 'qPFJetTopCHS0Pt', 'qPFJetTopCHS1Pt', 'qPFJetTopCHS2Pt', 'qPFJetTopCHS3Pt', 'qPFJetTopCHS4Pt', 'qPFJetTopCHS5Pt', 'qPFJetTopCHS0Eta', 'qPFJetTopCHS1Eta', 'qPFJetTopCHS2Eta', 'qPFJetTopCHS3Eta', 'qPFJetTopCHS4Eta', 'qPFJetTopCHS5Eta', 'qPFJetTopCHS0Phi', 'qPFJetTopCHS1Phi', 'qPFJetTopCHS2Phi', 'qPFJetTopCHS3Phi', 'qPFJetTopCHS4Phi', 'qPFJetTopCHS5Phi', 'qCalJet0Pt', 'qCalJet1Pt', 'qCalJet2Pt', 'qCalJet3Pt', 'qCalJet4Pt', 'qCalJet5Pt', 'qCalJet0Eta', 'qCalJet1Eta', 'qCalJet2Eta', 'qCalJet3Eta', 'qCalJet4Eta', 'qCalJet5Eta', 'qCalJet0Phi', 'qCalJet1Phi', 'qCalJet2Phi', 'qCalJet3Phi', 'qCalJet4Phi', 'qCalJet5Phi', 'qCalJet0En', 'qCalJet1En', 'qCalJet2En', 'qCalJet3En', 'qCalJet4En', 'qCalJet5En', 'qPho0Pt', 'qPho1Pt', 'qPho2Pt', 'qPho3Pt', 'qPho4Pt', 'qPho5Pt', 'qPho0Eta', 'qPho1Eta', 'qPho2Eta', 'qPho3Eta', 'qPho4Eta', 'qPho5Eta', 'qPho0Phi', 'qPho1Phi', 'qPho2Phi', 'qPho3Phi', 'qPho4Phi', 'qPho5Phi', 'qPho0En', 'qPho1En', 'qPho2En', 'qPho3En', 'qPho4En', 'qPho5En', 'qgedPho0Pt', 'qgedPho1Pt', 'qgedPho2Pt', 'qgedPho3Pt', 'qgedPho4Pt', 'qgedPho5Pt', 'qgedPho0Eta', 'qgedPho1Eta', 'qgedPho2Eta', 'qgedPho3Eta', 'qgedPho4Eta', 'qgedPho5Eta', 'qgedPho0Phi', 'qgedPho1Phi', 'qgedPho2Phi', 'qgedPho3Phi', 'qgedPho4Phi', 'qgedPho5Phi', 'qgedPho0En', 'qgedPho1En', 'qgedPho2En', 'qgedPho3En', 'qgedPho4En', 'qgedPho5En', 'qMu0Pt', 'qMu1Pt', 'qMu2Pt', 'qMu3Pt', 'qMu4Pt', 'qMu5Pt', 'qMu0Eta', 'qMu1Eta', 'qMu2Eta', 'qMu3Eta', 'qMu4Eta', 'qMu5Eta', 'qMu0Phi', 'qMu1Phi', 'qMu2Phi', 'qMu3Phi', 'qMu4Phi', 'qMu5Phi', 'qMu0En', 'qMu1En', 'qMu2En', 'qMu3En', 'qMu4En', 'qMu5En', 'qMuCosm0Pt', 'qMuCosm1Pt', 'qMuCosm2Pt', 'qMuCosm3Pt', 'qMuCosm4Pt', 'qMuCosm5Pt', 'qMuCosm0Eta', 'qMuCosm1Eta', 'qMuCosm2Eta', 'qMuCosm3Eta', 'qMuCosm4Eta', 'qMuCosm5Eta', 'qMuCosm0Phi', 'qMuCosm1Phi', 'qMuCosm2Phi', 'qMuCosm3Phi', 'qMuCosm4Phi', 'qMuCosm5Phi', 'qMuCosm0En', 'qMuCosm1En', 'qMuCosm2En', 'qMuCosm3En', 'qMuCosm4En', 'qMuCosm5En', 'qMuCosmLeg0Pt', 'qMuCosmLeg1Pt', 'qMuCosmLeg2Pt', 'qMuCosmLeg3Pt', 'qMuCosmLeg4Pt', 'qMuCosmLeg5Pt', 'qMuCosmLeg0Eta', 'qMuCosmLeg1Eta', 'qMuCosmLeg2Eta', 'qMuCosmLeg3Eta', 'qMuCosmLeg4Eta', 'qMuCosmLeg5Eta', 'qMuCosmLeg0Phi', 'qMuCosmLeg1Phi', 'qMuCosmLeg2Phi', 'qMuCosmLeg3Phi', 'qMuCosmLeg4Phi', 'qMuCosmLeg5Phi', 'qMuCosmLeg0En', 'qMuCosmLeg1En', 'qMuCosmLeg2En', 'qMuCosmLeg3En', 'qMuCosmLeg4En', 'qMuCosmLeg5En', 'qPFJet4CHSPt', 'qPFJet4CHSEta', 'qPFJet4CHSPhi', 'qPFJet8CHSPt', 'qPFJet8CHSEta', 'qPFJet8CHSPhi', 'qPFJetEIPt', 'qPFJetEIEta', 'qPFJetEIPhi', 'qPFJet8CHSSDPt', 'qPFJet8CHSSDEta', 'qPFJet8CHSSDPhi', 'qPFJetTopCHSPt', 'qPFJetTopCHSEta', 'qPFJetTopCHSPhi', 'qPFChMetPt', 'qPFChMetPhi', 'qPFMetPt', 'qPFMetPhi', 'qNVtx', 'qCalJetPt', 'qCalJetEta', 'qCalJetPhi', 'qCalJetEn', 'qCalMETPt', 'qCalMETPhi', 'qCalMETEn', 'qCalMETBEPt', 'qCalMETBEPhi', 'qCalMETBEEn', 'qCalMETBEFOPt', 'qCalMETBEFOPhi', 'qCalMETBEFOEn', 'qCalMETMPt', 'qCalMETMPhi', 'qCalMETMEn', 'qSCEn', 'qSCEta', 'qSCPhi', 'qSCEtaWidth', 'qSCPhiWidth', 'qSCEnhfEM', 'qSCEtahfEM', 'qSCPhihfEM', 'qSCEn5x5', 'qSCEta5x5', 'qSCPhi5x5', 'qSCEtaWidth5x5', 'qSCPhiWidth5x5', 'qCCEn', 'qCCEta', 'qCCPhi', 'qCCEn5x5', 'qCCEta5x5', 'qCCPhi5x5', 'qPhoPt', 'qPhoEta', 'qPhoPhi', 'qPhoEn_', 'qPhoe1x5_', 'qPhoe2x5_', 'qPhoe3x3_', 'qPhoe5x5_', 'qPhomaxenxtal_', 'qPhosigmaeta_', 'qPhosigmaIeta_', 'qPhor1x5_', 'qPhor2x5_', 'qPhor9_', 'qgedPhoPt', 'qgedPhoEta', 'qgedPhoPhi', 'qgedPhoEn_', 'qgedPhoe1x5_', 'qgedPhoe2x5_', 'qgedPhoe3x3_', 'qgedPhoe5x5_', 'qgedPhomaxenxtal_', 'qgedPhosigmaeta_', 'qgedPhosigmaIeta_', 'qgedPhor1x5_', 'qgedPhor2x5_', 'qgedPhor9_', 'qMuPt', 'qMuEta', 'qMuPhi', 'qMuEn_', 'qMuCh_', 'qMuChi2_', 'qMuCosmPt', 'qMuCosmEta', 'qMuCosmPhi', 'qMuCosmEn_', 'qMuCosmCh_', 'qMuCosmChi2_', 'qMuCosmLegPt', 'qMuCosmLegEta', 'qMuCosmLegPhi', 'qMuCosmLegEn_', 'qMuCosmLegCh_', 'qMuCosmLegChi2_', 'qSigmaIEta', 'qSigmaIPhi', 'qr9', 'qHadOEm', 'qdrSumPt', 'qdrSumEt', 'qeSCOP', 'qecEn', 'qUNSigmaIEta', 'qUNSigmaIPhi', 'qUNr9', 'qUNHadOEm', 'qUNdrSumPt', 'qUNdrSumEt', 'qUNeSCOP', 'qUNecEn', 'qEBenergy', 'qEBtime', 'qEBchi2', 'qEBiEta', 'qEBiPhi', 'qEEenergy', 'qEEtime', 'qEEchi2', 'qEEix', 'qEEiy', 'qESenergy', 'qEStime', 'qESix', 'qESiy', 'qHBHEenergy', 'qHBHEtime', 'qHBHEauxe', 'qHBHEieta', 'qHBHEiphi', 'qHFenergy', 'qHFtime', 'qHFieta', 'qHFiphi', 'qPreShEn', 'qPreShEta', 'qPreShPhi', 'qPreShYEn', 'qPreShYEta', 'qPreShYPhi', 'inst_luminosity', 'pileup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory names, be sure to provide a directory to store the model and change the label file when changing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = \"/eos/project/c/cmsml4dc/fsiroky/consistentlumih5/\"\n",
    "label_file = \"/eos/project/c/cmsml4dc/jsons/JetHT.json\"\n",
    "pileup_file = \"/afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions16/13TeV/PileUp/pileup_latest.txt\"\n",
    "model_directory = \"put something here\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_file_list(directory, pds, npd, typeof, extension):\n",
    "    files = []\n",
    "    parts = [\"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    for p in parts:\n",
    "        files.append(\"%s%s_%s_%s%s\" % (directory, pds[npd], p, typeof, extension))\n",
    "    return files\n",
    "\n",
    "files = get_file_list(data_directory, PDs, nPD, \"background\", \".h5\")\n",
    "files = files + get_file_list(data_directory, PDs, nPD, \"signal\", \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load good and bad jets\n",
    "def get_data(files):\n",
    "    readout = np.empty([0,2813])\n",
    "    \n",
    "    for file in files:\n",
    "        jet = file.split(\"/\")[-1][:-3]\n",
    "        print(\"Reading: %s\" % jet)\n",
    "        try:\n",
    "            h5file = h5py.File(file, \"r\")\n",
    "            readout = np.concatenate((readout, h5file[jet][:]), axis=0)\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have %s. %s\" % (jet, error))\n",
    "            continue\n",
    "\n",
    "    return readout\n",
    "\n",
    "data = pd.DataFrame(get_data(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"run\"] = data[2807].astype(int)\n",
    "data[\"lumi\"] = data[2808].astype(int)\n",
    "data[\"inst_lumi\"] = data[2809].astype(float)\n",
    "\n",
    "# Drop unnecessary meta data\n",
    "data.drop([2808, 2809, 2810, 2811, 2812], axis=1, inplace=True)\n",
    "\n",
    "# Append inst. luminosity at the end as well\n",
    "data[2807] = data[\"inst_lumi\"]\n",
    "\n",
    "# Sort by runID and then by lumiID\n",
    "data = data.sort_values([\"run\", \"lumi\"], ascending=[True,True])\n",
    "\n",
    "# Reset index\n",
    "data = data.reset_index(drop=True)  \n",
    "\n",
    "runIDs  = data[\"run\"].astype(int)\n",
    "lumiIDs = data[\"lumi\"].astype(int)\n",
    "luminosity = data[\"inst_lumi\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending pileup to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(pileup_file) as f:\n",
    "    pileup = json.load(f)\n",
    "\n",
    "def pileup_from_json(json_file, orig_runid, orig_lumid):\n",
    "    try:\n",
    "        for i in json_file[str(int(orig_runid))]:\n",
    "            if orig_lumid == i[0]:\n",
    "                return i[3]\n",
    "    except KeyError:\n",
    "        print('key error')\n",
    "    return 1\n",
    "\n",
    "def add_pileup(sample):\n",
    "    return pileup_from_json(pileup, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "data[\"pileup\"] = data.apply(add_pileup, axis=1)\n",
    "data[2808] = data[\"pileup\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending labels to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply labels\n",
    "output_json = json.load(open(label_file))\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    try:\n",
    "        for i in json_file[str(int(orig_runid))]:\n",
    "            if orig_lumid >= i[0] and orig_lumid <= i[1]:\n",
    "                return 0\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return 1\n",
    "\n",
    "def add_flags(sample):\n",
    "    return json_checker(output_json, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "data[\"label\"] = data.apply(add_flags, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to choose split of training/testing. In this code, the training set is the first 80% of data, and test set is the last 20% (chronologically). Then, the data is normalized using StandardScaler and bad lumisections are excluded from training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "TRAIN_FROM = round(0.0*len(data))\n",
    "TRAIN_TO = round(0.8*len(data))\n",
    "TEST_FROM = round(0.8*len(data))\n",
    "\n",
    "train = data.iloc[TRAIN_FROM:TRAIN_TO]\n",
    "X_train = pd.concat([train.iloc[:, :2808], train[2808]], axis=1)\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "test = data.iloc[TEST_FROM:]\n",
    "X_test = pd.concat([test.iloc[:, :2808], test[2808]], axis=1)\n",
    "y_test = test[\"label\"]\n",
    "\n",
    "# Normalization\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "# Train only on good\n",
    "X_train = X_train[y_train == 0]\n",
    "X_train_norm = X_train_norm[y_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 2809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(INPUT_DIM, ))\n",
    "\n",
    "x = Dense(2048, kernel_regularizer=l1_l2(10e-5))(input_layer)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(1024, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(64, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(1024, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(2048, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(INPUT_DIM)(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adamm = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_name)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALID_SPLIT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train_norm,\n",
    "                X_train_norm,\n",
    "                epochs=2048,\n",
    "                batch_size=256,\n",
    "                validation_split=VALID_SPLIT,\n",
    "                verbose=2,\n",
    "                callbacks=[early_stopper, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload saved model\n",
    "autoencoder = load_model(\"%s%s.h5\" % (model_directory, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "ae_pred = autoencoder.predict(X_test_norm)\n",
    "ae_pred_train = autoencoder.predict(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exclude validation set\n",
    "ae_pred_train = ae_pred_train[:int(1-VALID_SPLIT * len(ae_pred_train)), :]\n",
    "X_train_wo_valid = X_train_norm[:int(1-VALID_SPLIT * len(X_train_norm)), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different ways of calculating reconstruction error (allmean - MSE of all features, topn - MSE of n_highest worst reconstructed features, perobj - maximum of MSEs calculated for groups of features according to the legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_error_df(X_test, predictions, mode=\"allmean\", n_highest = 100):\n",
    "    \n",
    "    if mode == \"allmean\":\n",
    "        return np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "    \n",
    "    elif mode == \"topn\":\n",
    "        temp = np.partition(-np.power(X_test - predictions, 2), n_highest)\n",
    "        result = -temp[:,:n_highest]\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    elif mode == \"perobj\":\n",
    "        mses = []\n",
    "        for l in legend:\n",
    "            mse = np.mean(\n",
    "                np.power(X_test[:,legend[\"start\"]:legend[\"end\"]] - \n",
    "                         predictions[:,legend[\"start\"]:legend[\"end\"]], 2),\n",
    "                axis=1)\n",
    "            mses.append(mse)\n",
    "     \n",
    "        return np.maximum.reduce(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_error = get_error_df(X_test_norm, ae_pred, mode=\"topn\", n_highest=100)\n",
    "ae_error_train = get_error_df(X_train_wo_valid, ae_pred_train, mode=\"topn\", n_highest=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Pearson correlation coefficient for input and reconstruction per feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = [pearsonr(X_train_wo_valid[:, i].reshape(-1), ae_pred_train[:, i].reshape(-1)) for i in range(INPUT_DIM)]\n",
    "corr = np.array(corr)[:, 0]\n",
    "corr[np.isnan(corr)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(corr, bins=100)\n",
    "plt.title('Histogram of correlation coefficients for 2809 features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping features scoring below this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORR_FILTER = 0.5\n",
    "corr_filter = corr > CORR_FILTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping also features whose variance across lumisections is zero (they are constant-valued):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variance = np.var(data.iloc[:, 0:INPUT_DIM])\n",
    "var_filter = variance != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_filter = corr_filter & var_filter\n",
    "X_train_filtered = X_train_norm[:, feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_filtered_wo_valid = X_train.iloc[:round(0.75*len(X_train)), :]\n",
    "X_train_filtered_wo_valid = np.array(X_train_filtered_wo_valid)[:, feature_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of features and their correlation coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_names = []\n",
    "for i in tnrange(2809):\n",
    "    v, r = divmod(i, 7)\n",
    "    corr_names.append([feature_names[v],\n",
    "                      var_legend[r]['name'],\n",
    "                      corr[i]])\n",
    "\n",
    "corr_names_pd = pd.DataFrame(corr_names, columns=['name', 'group',                                                  'corr_coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller autoencoder to be trained on filtered subset of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_dim = X_train_filtered.shape[1]\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "x = Dense(1024, kernel_regularizer=l1_l2(10e-5))(input_layer)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(64, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(32, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(64, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(1024, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "x = Dense(input_dim)(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name2 = 'model2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adamm = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_name2)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train_filtered,\n",
    "                X_train_filtered,\n",
    "                epochs=2048,\n",
    "                batch_size=256,\n",
    "                validation_split=VALID_SPLIT,\n",
    "                verbose=2,\n",
    "                callbacks=[early_stopper, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload saved model\n",
    "autoencoder = load_model(\"%s%s.h5\" % (model_directory, model_name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_filtered = X_test_norm[:, feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude validation set\n",
    "X_train_wo_valid_filtered = X_train_wo_valid[:, feature_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "ae_pred_filtered = autoencoder.predict(X_test_filtered)\n",
    "ae_pred_filtered_train = autoencoder.predict(X_train_wo_valid_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ae_error_filtered = get_error_df(X_test_filtered, ae_pred_filtered, mode=\"topn\", n_highest=100)\n",
    "ae_error_filtered_train = get_error_df(X_train_wo_valid_filtered, ae_pred_filtered_train, mode=\"topn\", n_highest=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function plotting reconstruction error vs. time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot lumi vs error\n",
    "def plot_lumi_error(pred, X_test, threshold=None, title=None):\n",
    "    scores = get_error_df(X_test, pred, mode='topn')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.plot(np.array([i for i in range(len(scores))])[y_test == 0],\n",
    "             scores[y_test == 0],\n",
    "             linestyle='',\n",
    "             ms=2,\n",
    "             marker='o',\n",
    "             label=\"Outlier\")\n",
    "\n",
    "    plt.plot(np.array([i for i in range(len(scores))])[y_test == 1],\n",
    "             scores[y_test == 1],\n",
    "             linestyle='',\n",
    "             ms=2,\n",
    "             marker='o',\n",
    "             label=\"Inlier\")\n",
    "\n",
    "    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Lumisection #\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function plotting ROC curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make ROC_curve\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score                          \n",
    "\n",
    "def get_roc_curve(label, scores, names):\n",
    "    \"\"\"Generates ROC Curves for a given array\"\"\"    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        fpr, tpr, thresholds = roc_curve(label, scores[i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr,\n",
    "                 tpr,\n",
    "                 linewidth=3,\n",
    "                 label=(\"%s AUC: %s\" % (names[i], roc_auc)))\n",
    "        \n",
    "    plt.legend(frameon=False)\n",
    "    plt.ylabel(\"Sensitivity (TPR)\")\n",
    "    plt.xlabel(\"Fall-out (TNR)\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.show()\n",
    "    return plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
