{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "# Running on GPU?\n",
    "import setGPU\n",
    "\n",
    "import getpass\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get permission to access EOS (Insert your NICE password)\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense, Lambda, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.activations import sigmoid, linear, relu\n",
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PDs  = {1: 'BTagCSV',\n",
    "        2: 'BTagMu',\n",
    "        3: 'Charmonium',\n",
    "        4: 'DisplacedJet',\n",
    "        5: 'DoubleEG',\n",
    "        6: 'DoubleMuon',\n",
    "        7: 'DoubleMuonLowMass',\n",
    "        8: 'FSQJets',\n",
    "        9: 'HighMultiplicityEOF',\n",
    "        10: 'HTMHT',\n",
    "        11: 'JetHT',\n",
    "        12: 'MET',\n",
    "        13: 'MinimumBias',\n",
    "        14: 'MuonEG',\n",
    "        15: 'MuOnia',\n",
    "        16: 'NoBPTX',\n",
    "        17: 'SingleElectron',\n",
    "        18: 'SingleMuon',\n",
    "        19: 'SinglePhoton',\n",
    "        20: 'Tau',\n",
    "        21: 'ZeroBias'}\n",
    "\n",
    "# Select PD\n",
    "nPD = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directory = \"/eos/cms/store/user/fsiroky/consistentlumih5/\"\n",
    "label_file = \"/afs/cern.ch/user/t/tkrzyzek/Documents/Data-Certification/JetHT.json\"\n",
    "model_directory = \"/eos/user/t/tkrzyzek/autoencoder/batch_norm/\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(directory, pds, npd, typeof, extension):\n",
    "    files = []\n",
    "    parts = [\"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    for p in parts:\n",
    "        files.append(\"%s%s_%s_%s%s\" % (directory, pds[npd], p, typeof, extension))\n",
    "    return files\n",
    "\n",
    "files = get_file_list(data_directory, PDs, nPD, \"background\", \".h5\")\n",
    "files = files + get_file_list(data_directory, PDs, nPD, \"signal\", \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: JetHT_C_background\n",
      "Reading: JetHT_D_background\n",
      "Reading: JetHT_E_background\n",
      "Reading: JetHT_F_background\n",
      "Reading: JetHT_G_background\n",
      "Reading: JetHT_H_background\n",
      "Reading: JetHT_C_signal\n",
      "Reading: JetHT_D_signal\n",
      "Reading: JetHT_E_signal\n",
      "Reading: JetHT_F_signal\n",
      "Reading: JetHT_G_signal\n",
      "Reading: JetHT_H_signal\n"
     ]
    }
   ],
   "source": [
    "# Load good and bad jets\n",
    "def get_data(files):\n",
    "    readout = np.empty([0,2813])\n",
    "    \n",
    "    for file in files:\n",
    "        jet = file.split(\"/\")[-1][:-3]\n",
    "        print(\"Reading: %s\" % jet)\n",
    "        try:\n",
    "            h5file = h5py.File(file, \"r\")\n",
    "            readout = np.concatenate((readout, h5file[jet][:]), axis=0)\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have %s. %s\" % (jet, error))\n",
    "            continue\n",
    "\n",
    "    return readout\n",
    "\n",
    "data = pd.DataFrame(get_data(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"run\"] = data[2807].astype(int)\n",
    "data[\"lumi\"] = data[2808].astype(int)\n",
    "data[\"inst_lumi\"] = data[2809].astype(float)\n",
    "\n",
    "# Drop unnecessary meta data\n",
    "data.drop([2807, 2808, 2809, 2810, 2811, 2812], axis=1, inplace=True)\n",
    "\n",
    "# Sort by runID and then by lumiID\n",
    "data = data.sort_values([\"run\", \"lumi\"], ascending=[True,True])\n",
    "\n",
    "# Reset index\n",
    "data = data.reset_index(drop=True)  \n",
    "\n",
    "runIDs  = data[\"run\"].astype(int)\n",
    "lumiIDs = data[\"lumi\"].astype(int)\n",
    "luminosity = data[\"inst_lumi\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply labels\n",
    "output_json = json.load(open(label_file))\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    try:\n",
    "        for i in json_file[str(int(orig_runid))]:\n",
    "            if orig_lumid >= i[0] and orig_lumid <= i[1]:\n",
    "                return 0\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return 1\n",
    "\n",
    "def add_flags(sample):\n",
    "    return json_checker(output_json, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "data[\"label\"] = data.apply(add_flags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACPEAAASACAYAAACUOUo7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naCJcurIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafeca104e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "treshold = 4e6\n",
    "plt.rcParams['figure.figsize'] = [40, 20]\n",
    "plt.hist(data.iloc[:, :2807].values.reshape(-1), bins=1000)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.axvline(treshold, color='r')\n",
    "plt.ylabel(\"Feature value\")\n",
    "plt.xlabel(\"Number of feature values\")\n",
    "plt.title(\"Histogram of all feature values of all lumisections\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['keep'] = data.apply(lambda row: all([x < treshold for x in row]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(662, 2812)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['keep'] == False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_orig = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (16368, 2812)\n",
      "After: (16311, 2812)\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "SPLIT_FACTOR = 0.1\n",
    "\n",
    "split = round(SPLIT_FACTOR*len(data))\n",
    "\n",
    "runIDs = runIDs[split:]\n",
    "lumiIDs = lumiIDs[split:]\n",
    "luminosity = luminosity[split:]\n",
    "\n",
    "train = data.iloc[:split]\n",
    "print(\"Before:\", train.shape)\n",
    "train = train[train['keep'] == True]\n",
    "print(\"After:\", train.shape)\n",
    "X_train = train.iloc[:, 0:2806]\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "test = data.iloc[split:]\n",
    "X_test = test.iloc[:, 0:2806]\n",
    "y_test = test[\"label\"]\n",
    "    \n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "# Train only on good\n",
    "X_train = X_train[y_train == 0]\n",
    "X_train_norm = X_train_norm[y_train == 0]\n",
    "\n",
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 2806)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 2000)              5614000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_31 (PReLU)           (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_32 (PReLU)           (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "p_re_lu_33 (PReLU)           (None, 500)               500       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "p_re_lu_34 (PReLU)           (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 2000)              2002000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_35 (PReLU)           (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 2806)              5614806   \n",
      "=================================================================\n",
      "Total params: 16,239,806\n",
      "Trainable params: 16,239,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "# x = BatchNormalization()(input_layer)\n",
    "x = Dense(2000, kernel_regularizer=l1_l2(10e-5))(input_layer)\n",
    "x = PReLU()(x)\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "x = Dense(1000, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "x = Dense(500, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "x = Dense(1000, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "x = Dense(2000, kernel_regularizer=l1_l2(10e-5))(x)\n",
    "x = PReLU()(x)\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "x = Dense(input_dim)(x)\n",
    "x = linear(x)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adamm = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "# adamm = keras.optimizers.Adam(lr=0.001, epsilon=1e-08, decay=1e-05)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_name)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11818 samples, validate on 3940 samples\n",
      "Epoch 1/2048\n",
      "6s - loss: 69942.1094 - val_loss: 760.9656\n",
      "Epoch 2/2048\n",
      "4s - loss: 675.6330 - val_loss: 232.6206\n",
      "Epoch 3/2048\n",
      "3s - loss: 251.6050 - val_loss: 135.5671\n",
      "Epoch 4/2048\n",
      "3s - loss: 149.4670 - val_loss: 100.3357\n",
      "Epoch 5/2048\n",
      "3s - loss: 107.1848 - val_loss: 86.6091\n",
      "Epoch 6/2048\n",
      "3s - loss: 90.9596 - val_loss: 81.2403\n",
      "Epoch 7/2048\n",
      "3s - loss: 82.5716 - val_loss: 77.9918\n",
      "Epoch 8/2048\n",
      "5s - loss: 78.0719 - val_loss: 75.7936\n",
      "Epoch 9/2048\n",
      "4s - loss: 75.3410 - val_loss: 74.1682\n",
      "Epoch 10/2048\n",
      "4s - loss: 73.5791 - val_loss: 72.9480\n",
      "Epoch 11/2048\n",
      "3s - loss: 72.3196 - val_loss: 71.9938\n",
      "Epoch 12/2048\n",
      "4s - loss: 71.4191 - val_loss: 71.2076\n",
      "Epoch 13/2048\n",
      "3s - loss: 70.6933 - val_loss: 70.5665\n",
      "Epoch 14/2048\n",
      "4s - loss: 70.0774 - val_loss: 69.9693\n",
      "Epoch 15/2048\n",
      "4s - loss: 69.5337 - val_loss: 69.4311\n",
      "Epoch 16/2048\n",
      "3s - loss: 69.0382 - val_loss: 68.9374\n",
      "Epoch 17/2048\n",
      "4s - loss: 68.5747 - val_loss: 68.4743\n",
      "Epoch 18/2048\n",
      "3s - loss: 68.1381 - val_loss: 68.0340\n",
      "Epoch 19/2048\n",
      "4s - loss: 67.7214 - val_loss: 67.6119\n",
      "Epoch 20/2048\n",
      "4s - loss: 67.3203 - val_loss: 67.2042\n",
      "Epoch 21/2048\n",
      "4s - loss: 66.9315 - val_loss: 66.8136\n",
      "Epoch 22/2048\n",
      "8s - loss: 66.5551 - val_loss: 66.4367\n",
      "Epoch 23/2048\n",
      "17s - loss: 66.1892 - val_loss: 66.0705\n",
      "Epoch 24/2048\n",
      "4s - loss: 65.8317 - val_loss: 65.7146\n",
      "Epoch 25/2048\n",
      "18s - loss: 65.4839 - val_loss: 65.3661\n",
      "Epoch 26/2048\n",
      "18s - loss: 65.1441 - val_loss: 65.0271\n",
      "Epoch 27/2048\n",
      "17s - loss: 64.8121 - val_loss: 64.6975\n",
      "Epoch 28/2048\n",
      "4s - loss: 64.4873 - val_loss: 64.3746\n",
      "Epoch 29/2048\n",
      "14s - loss: 64.1687 - val_loss: 64.0534\n",
      "Epoch 30/2048\n",
      "13s - loss: 63.8573 - val_loss: 63.7440\n",
      "Epoch 31/2048\n",
      "12s - loss: 63.5518 - val_loss: 63.4413\n",
      "Epoch 32/2048\n",
      "4s - loss: 63.2522 - val_loss: 63.1443\n",
      "Epoch 33/2048\n",
      "8s - loss: 62.9583 - val_loss: 62.8525\n",
      "Epoch 34/2048\n",
      "9s - loss: 62.6698 - val_loss: 62.5666\n",
      "Epoch 35/2048\n",
      "7s - loss: 62.3865 - val_loss: 62.2856\n",
      "Epoch 36/2048\n",
      "13s - loss: 62.1082 - val_loss: 62.0095\n",
      "Epoch 37/2048\n",
      "11s - loss: 61.8347 - val_loss: 61.7383\n",
      "Epoch 38/2048\n",
      "11s - loss: 61.5659 - val_loss: 61.4718\n",
      "Epoch 39/2048\n",
      "7s - loss: 61.3016 - val_loss: 61.2087\n",
      "Epoch 40/2048\n",
      "11s - loss: 61.0417 - val_loss: 60.9509\n",
      "Epoch 41/2048\n",
      "8s - loss: 60.7861 - val_loss: 60.6973\n",
      "Epoch 42/2048\n",
      "4s - loss: 60.5346 - val_loss: 60.4478\n",
      "Epoch 43/2048\n",
      "5s - loss: 60.2871 - val_loss: 60.2023\n",
      "Epoch 44/2048\n",
      "4s - loss: 60.0434 - val_loss: 59.9607\n",
      "Epoch 45/2048\n",
      "4s - loss: 59.8035 - val_loss: 59.7226\n",
      "Epoch 46/2048\n",
      "4s - loss: 59.5673 - val_loss: 59.4883\n",
      "Epoch 47/2048\n",
      "4s - loss: 59.3347 - val_loss: 59.2574\n",
      "Epoch 48/2048\n",
      "3s - loss: 59.1055 - val_loss: 59.0301\n",
      "Epoch 49/2048\n",
      "4s - loss: 58.8797 - val_loss: 58.8063\n",
      "Epoch 50/2048\n",
      "4s - loss: 58.6571 - val_loss: 58.5852\n",
      "Epoch 51/2048\n",
      "3s - loss: 58.4377 - val_loss: 58.3673\n",
      "Epoch 52/2048\n",
      "3s - loss: 58.2214 - val_loss: 58.1527\n",
      "Epoch 53/2048\n",
      "6s - loss: 58.0081 - val_loss: 57.9413\n",
      "Epoch 54/2048\n",
      "4s - loss: 57.7977 - val_loss: 57.7318\n",
      "Epoch 55/2048\n",
      "5s - loss: 57.5901 - val_loss: 57.5257\n",
      "Epoch 56/2048\n",
      "4s - loss: 57.3854 - val_loss: 57.3226\n",
      "Epoch 57/2048\n",
      "4s - loss: 57.1833 - val_loss: 57.1216\n",
      "Epoch 58/2048\n",
      "4s - loss: 56.9838 - val_loss: 56.9238\n",
      "Epoch 59/2048\n",
      "5s - loss: 56.7868 - val_loss: 56.7282\n",
      "Epoch 60/2048\n",
      "4s - loss: 56.5923 - val_loss: 56.5352\n",
      "Epoch 61/2048\n",
      "5s - loss: 56.4003 - val_loss: 56.3447\n",
      "Epoch 62/2048\n",
      "4s - loss: 56.2107 - val_loss: 56.1561\n",
      "Epoch 63/2048\n",
      "4s - loss: 56.0234 - val_loss: 55.9703\n",
      "Epoch 64/2048\n",
      "4s - loss: 55.8387 - val_loss: 55.7866\n",
      "Epoch 65/2048\n",
      "6s - loss: 55.6556 - val_loss: 55.6052\n",
      "Epoch 66/2048\n",
      "4s - loss: 55.4750 - val_loss: 55.4259\n",
      "Epoch 67/2048\n",
      "4s - loss: 55.2964 - val_loss: 55.2485\n",
      "Epoch 68/2048\n",
      "4s - loss: 55.1199 - val_loss: 55.0731\n",
      "Epoch 69/2048\n",
      "4s - loss: 54.9454 - val_loss: 54.8998\n",
      "Epoch 70/2048\n",
      "3s - loss: 54.7728 - val_loss: 54.7287\n",
      "Epoch 71/2048\n",
      "4s - loss: 54.6022 - val_loss: 54.5589\n",
      "Epoch 72/2048\n",
      "4s - loss: 54.4334 - val_loss: 54.3911\n",
      "Epoch 73/2048\n",
      "4s - loss: 54.2664 - val_loss: 54.2252\n",
      "Epoch 74/2048\n",
      "4s - loss: 54.1014 - val_loss: 54.0608\n",
      "Epoch 75/2048\n",
      "3s - loss: 53.9379 - val_loss: 53.8991\n",
      "Epoch 76/2048\n",
      "3s - loss: 53.7762 - val_loss: 53.7380\n",
      "Epoch 77/2048\n",
      "4s - loss: 53.6161 - val_loss: 53.5793\n",
      "Epoch 78/2048\n",
      "4s - loss: 53.4576 - val_loss: 53.4213\n",
      "Epoch 79/2048\n",
      "3s - loss: 53.3008 - val_loss: 53.2659\n",
      "Epoch 80/2048\n",
      "3s - loss: 53.1455 - val_loss: 53.1116\n",
      "Epoch 81/2048\n",
      "4s - loss: 52.9919 - val_loss: 52.9540\n",
      "Epoch 82/2048\n",
      "7s - loss: 52.8398 - val_loss: 52.8035\n",
      "Epoch 83/2048\n",
      "4s - loss: 52.6889 - val_loss: 52.6545\n",
      "Epoch 84/2048\n",
      "3s - loss: 52.5396 - val_loss: 52.5072\n",
      "Epoch 85/2048\n",
      "4s - loss: 52.3917 - val_loss: 52.3607\n",
      "Epoch 86/2048\n",
      "4s - loss: 52.2453 - val_loss: 52.2154\n",
      "Epoch 87/2048\n",
      "4s - loss: 52.0999 - val_loss: 52.0718\n",
      "Epoch 88/2048\n",
      "4s - loss: 51.9558 - val_loss: 51.9286\n",
      "Epoch 89/2048\n",
      "4s - loss: 51.8131 - val_loss: 51.7868\n",
      "Epoch 90/2048\n",
      "3s - loss: 51.6716 - val_loss: 51.6464\n",
      "Epoch 91/2048\n",
      "4s - loss: 51.5314 - val_loss: 51.5074\n",
      "Epoch 92/2048\n",
      "4s - loss: 51.3923 - val_loss: 51.3698\n",
      "Epoch 93/2048\n",
      "4s - loss: 51.2545 - val_loss: 51.2327\n",
      "Epoch 94/2048\n",
      "4s - loss: 51.1178 - val_loss: 51.0966\n",
      "Epoch 95/2048\n",
      "4s - loss: 50.9822 - val_loss: 50.9621\n",
      "Epoch 96/2048\n",
      "4s - loss: 50.8478 - val_loss: 50.8284\n",
      "Epoch 97/2048\n",
      "4s - loss: 50.7145 - val_loss: 50.6959\n",
      "Epoch 98/2048\n",
      "3s - loss: 50.5822 - val_loss: 50.5643\n",
      "Epoch 99/2048\n",
      "3s - loss: 50.4507 - val_loss: 50.4335\n",
      "Epoch 100/2048\n",
      "4s - loss: 50.3203 - val_loss: 50.3046\n",
      "Epoch 101/2048\n",
      "3s - loss: 50.1912 - val_loss: 50.1767\n",
      "Epoch 102/2048\n",
      "4s - loss: 50.0630 - val_loss: 50.0482\n",
      "Epoch 103/2048\n",
      "4s - loss: 49.9369 - val_loss: 49.9189\n",
      "Epoch 104/2048\n",
      "3s - loss: 49.8129 - val_loss: 49.7903\n",
      "Epoch 105/2048\n",
      "3s - loss: 49.6869 - val_loss: 49.6677\n",
      "Epoch 106/2048\n",
      "3s - loss: 49.5705 - val_loss: 49.5349\n",
      "Epoch 107/2048\n",
      "3s - loss: 49.4463 - val_loss: 49.4122\n",
      "Epoch 108/2048\n",
      "4s - loss: 49.3157 - val_loss: 49.2909\n",
      "Epoch 109/2048\n",
      "4s - loss: 49.1921 - val_loss: 49.1710\n",
      "Epoch 110/2048\n",
      "4s - loss: 49.0702 - val_loss: 49.0528\n",
      "Epoch 111/2048\n",
      "3s - loss: 48.9492 - val_loss: 48.9328\n",
      "Epoch 112/2048\n",
      "3s - loss: 48.8291 - val_loss: 48.8150\n",
      "Epoch 113/2048\n",
      "4s - loss: 48.7098 - val_loss: 48.6962\n",
      "Epoch 114/2048\n",
      "4s - loss: 48.5911 - val_loss: 48.5778\n",
      "Epoch 115/2048\n",
      "4s - loss: 48.4733 - val_loss: 48.4616\n",
      "Epoch 116/2048\n",
      "4s - loss: 48.3562 - val_loss: 48.3456\n",
      "Epoch 117/2048\n",
      "3s - loss: 48.2399 - val_loss: 48.2301\n",
      "Epoch 118/2048\n",
      "4s - loss: 48.1239 - val_loss: 48.1157\n",
      "Epoch 119/2048\n",
      "3s - loss: 48.0085 - val_loss: 47.9995\n",
      "Epoch 120/2048\n",
      "4s - loss: 47.8937 - val_loss: 47.8884\n",
      "Epoch 121/2048\n",
      "3s - loss: 47.7799 - val_loss: 47.7748\n",
      "Epoch 122/2048\n",
      "3s - loss: 47.6665 - val_loss: 47.6587\n",
      "Epoch 123/2048\n",
      "4s - loss: 47.5536 - val_loss: 47.5469\n",
      "Epoch 124/2048\n",
      "4s - loss: 47.4410 - val_loss: 47.4342\n",
      "Epoch 125/2048\n",
      "5s - loss: 47.3316 - val_loss: 47.3228\n",
      "Epoch 126/2048\n",
      "4s - loss: 47.2191 - val_loss: 47.2112\n",
      "Epoch 127/2048\n",
      "3s - loss: 47.1074 - val_loss: 47.1024\n",
      "Epoch 128/2048\n",
      "4s - loss: 46.9965 - val_loss: 46.9919\n",
      "Epoch 129/2048\n",
      "4s - loss: 46.8863 - val_loss: 46.8829\n",
      "Epoch 130/2048\n",
      "3s - loss: 46.7766 - val_loss: 46.7732\n",
      "Epoch 131/2048\n",
      "4s - loss: 46.6674 - val_loss: 46.6667\n",
      "Epoch 132/2048\n",
      "4s - loss: 46.5590 - val_loss: 46.5589\n",
      "Epoch 133/2048\n",
      "4s - loss: 46.4507 - val_loss: 46.4465\n",
      "Epoch 134/2048\n",
      "4s - loss: 46.3425 - val_loss: 46.3418\n",
      "Epoch 135/2048\n",
      "4s - loss: 46.2341 - val_loss: 46.2331\n",
      "Epoch 136/2048\n",
      "4s - loss: 46.1264 - val_loss: 46.1298\n",
      "Epoch 137/2048\n",
      "4s - loss: 46.0193 - val_loss: 46.0231\n",
      "Epoch 138/2048\n",
      "4s - loss: 45.9122 - val_loss: 45.9176\n",
      "Epoch 139/2048\n",
      "4s - loss: 45.8060 - val_loss: 45.8102\n",
      "Epoch 140/2048\n",
      "4s - loss: 45.6998 - val_loss: 45.7065\n",
      "Epoch 141/2048\n",
      "3s - loss: 45.5950 - val_loss: 45.5969\n",
      "Epoch 142/2048\n",
      "4s - loss: 45.4885 - val_loss: 45.4927\n",
      "Epoch 143/2048\n",
      "4s - loss: 45.3851 - val_loss: 45.3860\n",
      "Epoch 144/2048\n",
      "3s - loss: 45.2858 - val_loss: 45.2757\n",
      "Epoch 145/2048\n",
      "4s - loss: 45.2215 - val_loss: 45.1665\n",
      "Epoch 146/2048\n",
      "3s - loss: 45.0859 - val_loss: 45.0533\n",
      "Epoch 147/2048\n",
      "6s - loss: 45.0160 - val_loss: 44.9576\n",
      "Epoch 148/2048\n",
      "4s - loss: 44.8719 - val_loss: 44.8428\n",
      "Epoch 149/2048\n",
      "3s - loss: 44.7658 - val_loss: 44.7394\n",
      "Epoch 150/2048\n",
      "4s - loss: 44.6604 - val_loss: 44.6376\n",
      "Epoch 151/2048\n",
      "4s - loss: 44.5556 - val_loss: 44.5343\n",
      "Epoch 152/2048\n",
      "6s - loss: 44.4510 - val_loss: 44.4311\n",
      "Epoch 153/2048\n",
      "3s - loss: 44.3464 - val_loss: 44.3285\n",
      "Epoch 154/2048\n",
      "4s - loss: 44.2421 - val_loss: 44.2240\n",
      "Epoch 155/2048\n",
      "3s - loss: 44.1381 - val_loss: 44.1226\n",
      "Epoch 156/2048\n",
      "3s - loss: 44.0340 - val_loss: 44.0190\n",
      "Epoch 157/2048\n",
      "5s - loss: 43.9304 - val_loss: 43.9176\n",
      "Epoch 158/2048\n",
      "4s - loss: 43.8266 - val_loss: 43.8142\n",
      "Epoch 159/2048\n",
      "4s - loss: 43.7227 - val_loss: 43.7120\n",
      "Epoch 160/2048\n",
      "3s - loss: 43.6189 - val_loss: 43.6113\n",
      "Epoch 161/2048\n",
      "4s - loss: 43.5153 - val_loss: 43.5061\n",
      "Epoch 162/2048\n",
      "4s - loss: 43.4117 - val_loss: 43.4054\n",
      "Epoch 163/2048\n",
      "4s - loss: 43.3079 - val_loss: 43.3008\n",
      "Epoch 164/2048\n",
      "4s - loss: 43.2043 - val_loss: 43.1992\n",
      "Epoch 165/2048\n",
      "3s - loss: 43.1006 - val_loss: 43.0969\n",
      "Epoch 166/2048\n",
      "10s - loss: 42.9969 - val_loss: 42.9944\n",
      "Epoch 167/2048\n",
      "3s - loss: 42.8929 - val_loss: 42.8904\n",
      "Epoch 168/2048\n",
      "3s - loss: 42.7892 - val_loss: 42.7888\n",
      "Epoch 169/2048\n",
      "3s - loss: 42.6854 - val_loss: 42.6844\n",
      "Epoch 170/2048\n",
      "4s - loss: 42.5815 - val_loss: 42.5818\n",
      "Epoch 171/2048\n",
      "5s - loss: 42.4778 - val_loss: 42.4765\n",
      "Epoch 172/2048\n",
      "4s - loss: 42.3734 - val_loss: 42.3739\n",
      "Epoch 173/2048\n",
      "4s - loss: 42.2691 - val_loss: 42.2703\n",
      "Epoch 174/2048\n",
      "4s - loss: 42.1654 - val_loss: 42.1680\n",
      "Epoch 175/2048\n",
      "3s - loss: 42.0604 - val_loss: 42.0630\n",
      "Epoch 176/2048\n",
      "3s - loss: 41.9561 - val_loss: 41.9566\n",
      "Epoch 177/2048\n",
      "4s - loss: 41.8515 - val_loss: 41.8523\n",
      "Epoch 178/2048\n",
      "4s - loss: 41.7463 - val_loss: 41.7473\n",
      "Epoch 179/2048\n",
      "3s - loss: 41.6423 - val_loss: 41.6416\n",
      "Epoch 180/2048\n",
      "3s - loss: 41.5367 - val_loss: 41.5326\n",
      "Epoch 181/2048\n",
      "3s - loss: 41.4308 - val_loss: 41.4294\n",
      "Epoch 182/2048\n",
      "4s - loss: 41.3251 - val_loss: 41.3234\n",
      "Epoch 183/2048\n",
      "4s - loss: 41.2193 - val_loss: 41.2190\n",
      "Epoch 184/2048\n",
      "4s - loss: 41.1131 - val_loss: 41.1097\n",
      "Epoch 185/2048\n",
      "4s - loss: 41.0071 - val_loss: 41.0058\n",
      "Epoch 186/2048\n",
      "4s - loss: 40.9007 - val_loss: 40.8929\n",
      "Epoch 187/2048\n",
      "3s - loss: 40.7942 - val_loss: 40.7885\n",
      "Epoch 188/2048\n",
      "3s - loss: 40.6879 - val_loss: 40.6814\n",
      "Epoch 189/2048\n",
      "3s - loss: 40.5806 - val_loss: 40.5753\n",
      "Epoch 190/2048\n",
      "4s - loss: 40.4737 - val_loss: 40.4685\n",
      "Epoch 191/2048\n",
      "4s - loss: 40.3659 - val_loss: 40.3559\n",
      "Epoch 192/2048\n",
      "3s - loss: 40.2580 - val_loss: 40.2490\n",
      "Epoch 193/2048\n",
      "3s - loss: 40.1499 - val_loss: 40.1410\n",
      "Epoch 194/2048\n",
      "4s - loss: 40.0414 - val_loss: 40.0327\n",
      "Epoch 195/2048\n",
      "4s - loss: 39.9328 - val_loss: 39.9211\n",
      "Epoch 196/2048\n",
      "4s - loss: 39.8238 - val_loss: 39.8111\n",
      "Epoch 197/2048\n",
      "4s - loss: 39.7145 - val_loss: 39.7003\n",
      "Epoch 198/2048\n",
      "4s - loss: 39.6055 - val_loss: 39.5929\n",
      "Epoch 199/2048\n",
      "4s - loss: 39.4983 - val_loss: 39.4776\n",
      "Epoch 200/2048\n",
      "3s - loss: 39.3875 - val_loss: 39.3686\n",
      "Epoch 201/2048\n",
      "4s - loss: 39.2765 - val_loss: 39.2601\n",
      "Epoch 202/2048\n",
      "4s - loss: 39.1651 - val_loss: 39.1495\n",
      "Epoch 203/2048\n",
      "3s - loss: 39.0538 - val_loss: 39.0391\n",
      "Epoch 204/2048\n",
      "3s - loss: 38.9423 - val_loss: 38.9260\n",
      "Epoch 205/2048\n",
      "3s - loss: 38.8304 - val_loss: 38.8133\n",
      "Epoch 206/2048\n",
      "4s - loss: 38.7184 - val_loss: 38.7026\n",
      "Epoch 207/2048\n",
      "4s - loss: 38.6067 - val_loss: 38.5898\n",
      "Epoch 208/2048\n",
      "4s - loss: 38.4918 - val_loss: 38.4723\n",
      "Epoch 209/2048\n",
      "4s - loss: 38.3761 - val_loss: 38.3596\n",
      "Epoch 210/2048\n",
      "4s - loss: 38.2571 - val_loss: 38.2451\n",
      "Epoch 211/2048\n",
      "4s - loss: 38.1286 - val_loss: 38.1184\n",
      "Epoch 212/2048\n",
      "4s - loss: 37.9820 - val_loss: 37.9973\n",
      "Epoch 213/2048\n",
      "3s - loss: 37.8369 - val_loss: 37.8758\n",
      "Epoch 214/2048\n",
      "3s - loss: 37.7425 - val_loss: 37.7680\n",
      "Epoch 215/2048\n",
      "5s - loss: 37.6128 - val_loss: 37.6395\n",
      "Epoch 216/2048\n",
      "3s - loss: 37.4807 - val_loss: 37.5229\n",
      "Epoch 217/2048\n",
      "3s - loss: 37.3570 - val_loss: 37.4051\n",
      "Epoch 218/2048\n",
      "5s - loss: 37.2362 - val_loss: 37.2851\n",
      "Epoch 219/2048\n",
      "3s - loss: 37.1160 - val_loss: 37.1779\n",
      "Epoch 220/2048\n",
      "3s - loss: 37.0098 - val_loss: 37.0594\n",
      "Epoch 221/2048\n",
      "3s - loss: 36.8798 - val_loss: 36.9271\n",
      "Epoch 222/2048\n",
      "3s - loss: 36.7777 - val_loss: 36.8838\n",
      "Epoch 223/2048\n",
      "3s - loss: 36.7965 - val_loss: 36.7520\n",
      "Epoch 224/2048\n",
      "4s - loss: 36.6719 - val_loss: 36.6278\n",
      "Epoch 225/2048\n",
      "3s - loss: 36.5491 - val_loss: 36.5056\n",
      "Epoch 226/2048\n",
      "3s - loss: 36.4264 - val_loss: 36.3846\n",
      "Epoch 227/2048\n",
      "5s - loss: 36.3045 - val_loss: 36.2635\n",
      "Epoch 228/2048\n",
      "3s - loss: 36.1828 - val_loss: 36.1434\n",
      "Epoch 229/2048\n",
      "4s - loss: 36.0619 - val_loss: 36.0223\n",
      "Epoch 230/2048\n",
      "4s - loss: 35.9391 - val_loss: 35.9013\n",
      "Epoch 231/2048\n",
      "4s - loss: 35.8169 - val_loss: 35.7784\n",
      "Epoch 232/2048\n",
      "4s - loss: 35.6937 - val_loss: 35.6562\n",
      "Epoch 233/2048\n",
      "4s - loss: 35.5705 - val_loss: 35.5336\n",
      "Epoch 234/2048\n",
      "3s - loss: 35.4499 - val_loss: 35.4104\n",
      "Epoch 235/2048\n",
      "3s - loss: 35.3239 - val_loss: 35.2869\n",
      "Epoch 236/2048\n",
      "4s - loss: 35.1990 - val_loss: 35.1626\n",
      "Epoch 237/2048\n",
      "3s - loss: 35.0740 - val_loss: 35.0381\n",
      "Epoch 238/2048\n",
      "7s - loss: 34.9488 - val_loss: 34.9140\n",
      "Epoch 239/2048\n",
      "3s - loss: 34.8229 - val_loss: 34.7869\n",
      "Epoch 240/2048\n",
      "3s - loss: 34.6978 - val_loss: 34.6608\n",
      "Epoch 241/2048\n",
      "4s - loss: 34.5697 - val_loss: 34.5337\n",
      "Epoch 242/2048\n",
      "4s - loss: 34.4432 - val_loss: 34.4048\n",
      "Epoch 243/2048\n",
      "3s - loss: 34.3162 - val_loss: 34.2793\n",
      "Epoch 244/2048\n",
      "3s - loss: 34.1864 - val_loss: 34.1506\n",
      "Epoch 245/2048\n",
      "3s - loss: 34.0574 - val_loss: 34.0203\n",
      "Epoch 246/2048\n",
      "4s - loss: 33.9281 - val_loss: 33.8927\n",
      "Epoch 247/2048\n",
      "4s - loss: 33.7983 - val_loss: 33.7601\n",
      "Epoch 248/2048\n",
      "5s - loss: 33.6693 - val_loss: 33.6284\n",
      "Epoch 249/2048\n",
      "4s - loss: 33.5372 - val_loss: 33.4997\n",
      "Epoch 250/2048\n",
      "4s - loss: 33.4051 - val_loss: 33.3663\n",
      "Epoch 251/2048\n",
      "4s - loss: 33.2731 - val_loss: 33.2353\n",
      "Epoch 252/2048\n",
      "4s - loss: 33.1419 - val_loss: 33.1034\n",
      "Epoch 253/2048\n",
      "4s - loss: 33.0090 - val_loss: 32.9709\n",
      "Epoch 254/2048\n",
      "4s - loss: 32.8751 - val_loss: 32.8364\n",
      "Epoch 255/2048\n",
      "4s - loss: 32.7401 - val_loss: 32.7033\n",
      "Epoch 256/2048\n",
      "4s - loss: 32.6053 - val_loss: 32.5641\n",
      "Epoch 257/2048\n",
      "3s - loss: 32.4699 - val_loss: 32.4276\n",
      "Epoch 258/2048\n",
      "4s - loss: 32.3340 - val_loss: 32.2922\n",
      "Epoch 259/2048\n",
      "3s - loss: 32.1978 - val_loss: 32.1585\n",
      "Epoch 260/2048\n",
      "3s - loss: 32.0594 - val_loss: 32.0173\n",
      "Epoch 261/2048\n",
      "3s - loss: 31.9196 - val_loss: 31.8808\n",
      "Epoch 262/2048\n",
      "3s - loss: 31.7777 - val_loss: 31.7434\n",
      "Epoch 263/2048\n",
      "3s - loss: 31.6298 - val_loss: 31.6002\n",
      "Epoch 264/2048\n",
      "4s - loss: 31.4747 - val_loss: 31.4582\n",
      "Epoch 265/2048\n",
      "4s - loss: 31.3545 - val_loss: 31.3290\n",
      "Epoch 266/2048\n",
      "4s - loss: 31.1914 - val_loss: 31.1828\n",
      "Epoch 267/2048\n",
      "3s - loss: 31.0231 - val_loss: 31.0335\n",
      "Epoch 268/2048\n",
      "4s - loss: 30.8753 - val_loss: 30.8896\n",
      "Epoch 269/2048\n",
      "4s - loss: 30.7217 - val_loss: 30.7397\n",
      "Epoch 270/2048\n",
      "3s - loss: 30.6461 - val_loss: 30.6335\n",
      "Epoch 271/2048\n",
      "4s - loss: 30.4815 - val_loss: 30.4761\n",
      "Epoch 272/2048\n",
      "4s - loss: 30.3191 - val_loss: 30.3269\n",
      "Epoch 273/2048\n",
      "4s - loss: 30.1642 - val_loss: 30.1780\n",
      "Epoch 274/2048\n",
      "3s - loss: 30.0122 - val_loss: 30.0300\n",
      "Epoch 275/2048\n",
      "3s - loss: 29.8655 - val_loss: 29.8847\n",
      "Epoch 276/2048\n",
      "4s - loss: 29.7193 - val_loss: 29.7337\n",
      "Epoch 277/2048\n",
      "4s - loss: 29.5669 - val_loss: 29.5942\n",
      "Epoch 278/2048\n",
      "4s - loss: 29.4213 - val_loss: 29.4425\n",
      "Epoch 279/2048\n",
      "4s - loss: 29.2692 - val_loss: 29.3173\n",
      "Epoch 280/2048\n",
      "4s - loss: 29.1266 - val_loss: 29.1428\n",
      "Epoch 281/2048\n",
      "3s - loss: 28.9699 - val_loss: 28.9929\n",
      "Epoch 282/2048\n",
      "3s - loss: 28.8221 - val_loss: 28.8540\n",
      "Epoch 283/2048\n",
      "3s - loss: 28.7735 - val_loss: 28.7958\n",
      "Epoch 284/2048\n",
      "4s - loss: 28.7019 - val_loss: 28.6344\n",
      "Epoch 285/2048\n",
      "4s - loss: 28.5442 - val_loss: 28.4788\n",
      "Epoch 286/2048\n",
      "4s - loss: 28.3926 - val_loss: 28.3259\n",
      "Epoch 287/2048\n",
      "3s - loss: 28.2755 - val_loss: 28.1922\n",
      "Epoch 288/2048\n",
      "4s - loss: 28.1341 - val_loss: 28.0519\n",
      "Epoch 289/2048\n",
      "4s - loss: 27.9619 - val_loss: 27.8925\n",
      "Epoch 290/2048\n",
      "4s - loss: 27.8115 - val_loss: 27.7370\n",
      "Epoch 291/2048\n",
      "3s - loss: 27.6524 - val_loss: 27.5861\n",
      "Epoch 292/2048\n",
      "4s - loss: 27.4991 - val_loss: 27.4320\n",
      "Epoch 293/2048\n",
      "3s - loss: 27.3441 - val_loss: 27.2749\n",
      "Epoch 294/2048\n",
      "3s - loss: 27.1848 - val_loss: 27.1195\n",
      "Epoch 295/2048\n",
      "3s - loss: 27.0290 - val_loss: 26.9642\n",
      "Epoch 296/2048\n",
      "4s - loss: 26.8732 - val_loss: 26.8085\n",
      "Epoch 297/2048\n",
      "3s - loss: 26.7178 - val_loss: 26.6517\n",
      "Epoch 298/2048\n",
      "3s - loss: 26.5729 - val_loss: 26.4974\n",
      "Epoch 299/2048\n",
      "3s - loss: 26.4260 - val_loss: 26.4354\n",
      "Epoch 300/2048\n",
      "4s - loss: 26.2567 - val_loss: 26.1883\n",
      "Epoch 301/2048\n",
      "3s - loss: 26.0962 - val_loss: 26.0295\n",
      "Epoch 302/2048\n",
      "4s - loss: 25.9376 - val_loss: 25.8709\n",
      "Epoch 303/2048\n",
      "4s - loss: 25.7796 - val_loss: 25.7117\n",
      "Epoch 304/2048\n",
      "3s - loss: 25.6191 - val_loss: 25.5526\n",
      "Epoch 305/2048\n",
      "4s - loss: 25.4596 - val_loss: 25.3927\n",
      "Epoch 306/2048\n",
      "5s - loss: 25.3045 - val_loss: 25.2330\n",
      "Epoch 307/2048\n",
      "3s - loss: 25.1394 - val_loss: 25.0721\n",
      "Epoch 308/2048\n",
      "4s - loss: 24.9782 - val_loss: 24.9125\n",
      "Epoch 309/2048\n",
      "3s - loss: 24.8166 - val_loss: 24.7492\n",
      "Epoch 310/2048\n",
      "3s - loss: 24.6554 - val_loss: 24.5870\n",
      "Epoch 311/2048\n",
      "4s - loss: 24.4938 - val_loss: 24.4249\n",
      "Epoch 312/2048\n",
      "4s - loss: 24.3298 - val_loss: 24.2617\n",
      "Epoch 313/2048\n",
      "4s - loss: 24.1665 - val_loss: 24.0984\n",
      "Epoch 314/2048\n",
      "4s - loss: 24.0031 - val_loss: 23.9349\n",
      "Epoch 315/2048\n",
      "4s - loss: 23.8399 - val_loss: 23.7714\n",
      "Epoch 316/2048\n",
      "2s - loss: 23.9445 - val_loss: 23.7832\n",
      "Epoch 317/2048\n",
      "3s - loss: 23.6141 - val_loss: 23.5359\n",
      "Epoch 318/2048\n",
      "4s - loss: 23.4366 - val_loss: 23.3659\n",
      "Epoch 319/2048\n",
      "3s - loss: 23.2671 - val_loss: 23.1981\n",
      "Epoch 320/2048\n",
      "2s - loss: 23.1116 - val_loss: 24.1866\n",
      "Epoch 321/2048\n",
      "3s - loss: 22.9551 - val_loss: 22.8716\n",
      "Epoch 322/2048\n",
      "4s - loss: 22.7757 - val_loss: 22.7080\n",
      "Epoch 323/2048\n",
      "3s - loss: 22.6122 - val_loss: 22.5457\n",
      "Epoch 324/2048\n",
      "4s - loss: 22.4597 - val_loss: 22.3861\n",
      "Epoch 325/2048\n",
      "3s - loss: 22.3161 - val_loss: 22.2236\n",
      "Epoch 326/2048\n",
      "4s - loss: 22.1843 - val_loss: 22.0620\n",
      "Epoch 327/2048\n",
      "4s - loss: 21.9649 - val_loss: 21.8964\n",
      "Epoch 328/2048\n",
      "4s - loss: 21.7999 - val_loss: 21.7327\n",
      "Epoch 329/2048\n",
      "4s - loss: 21.6355 - val_loss: 21.5682\n",
      "Epoch 330/2048\n",
      "4s - loss: 21.4713 - val_loss: 21.4040\n",
      "Epoch 331/2048\n",
      "5s - loss: 21.3066 - val_loss: 21.2393\n",
      "Epoch 332/2048\n",
      "3s - loss: 21.1420 - val_loss: 21.0744\n",
      "Epoch 333/2048\n",
      "4s - loss: 20.9789 - val_loss: 20.9106\n",
      "Epoch 334/2048\n",
      "4s - loss: 20.8159 - val_loss: 20.7455\n",
      "Epoch 335/2048\n",
      "4s - loss: 20.6480 - val_loss: 20.6061\n",
      "Epoch 336/2048\n",
      "4s - loss: 20.4975 - val_loss: 20.4175\n",
      "Epoch 337/2048\n",
      "3s - loss: 20.3183 - val_loss: 20.2506\n",
      "Epoch 338/2048\n",
      "4s - loss: 20.1588 - val_loss: 20.0854\n",
      "Epoch 339/2048\n",
      "4s - loss: 19.9916 - val_loss: 19.9187\n",
      "Epoch 340/2048\n",
      "6s - loss: 19.8229 - val_loss: 19.7537\n",
      "Epoch 341/2048\n",
      "4s - loss: 19.6548 - val_loss: 19.5862\n",
      "Epoch 342/2048\n",
      "4s - loss: 19.4879 - val_loss: 19.4194\n",
      "Epoch 343/2048\n",
      "3s - loss: 19.3210 - val_loss: 19.2805\n",
      "Epoch 344/2048\n",
      "4s - loss: 19.1552 - val_loss: 19.0865\n",
      "Epoch 345/2048\n",
      "4s - loss: 18.9887 - val_loss: 18.9200\n",
      "Epoch 346/2048\n",
      "4s - loss: 18.8241 - val_loss: 18.7531\n",
      "Epoch 347/2048\n",
      "5s - loss: 18.6549 - val_loss: 18.5862\n",
      "Epoch 348/2048\n",
      "4s - loss: 18.4967 - val_loss: 18.4246\n",
      "Epoch 349/2048\n",
      "7s - loss: 18.3286 - val_loss: 18.2553\n",
      "Epoch 350/2048\n",
      "5s - loss: 18.1899 - val_loss: 18.2265\n",
      "Epoch 351/2048\n",
      "11s - loss: 18.0092 - val_loss: 17.9284\n",
      "Epoch 352/2048\n",
      "4s - loss: 17.8281 - val_loss: 17.7587\n",
      "Epoch 353/2048\n",
      "7s - loss: 17.6591 - val_loss: 17.5903\n",
      "Epoch 354/2048\n",
      "3s - loss: 17.4910 - val_loss: 17.4226\n",
      "Epoch 355/2048\n",
      "4s - loss: 17.3236 - val_loss: 17.2554\n",
      "Epoch 356/2048\n",
      "4s - loss: 17.1565 - val_loss: 17.0884\n",
      "Epoch 357/2048\n",
      "4s - loss: 16.9923 - val_loss: 16.9219\n",
      "Epoch 358/2048\n",
      "4s - loss: 16.8247 - val_loss: 16.7589\n",
      "Epoch 359/2048\n",
      "6s - loss: 16.6579 - val_loss: 16.5896\n",
      "Epoch 360/2048\n",
      "4s - loss: 16.5319 - val_loss: 16.4264\n",
      "Epoch 361/2048\n",
      "6s - loss: 16.3295 - val_loss: 16.2588\n",
      "Epoch 362/2048\n",
      "4s - loss: 16.1599 - val_loss: 16.0926\n",
      "Epoch 363/2048\n",
      "3s - loss: 16.0050 - val_loss: 15.9271\n",
      "Epoch 364/2048\n",
      "3s - loss: 15.8281 - val_loss: 15.7605\n",
      "Epoch 365/2048\n",
      "4s - loss: 15.6618 - val_loss: 15.5947\n",
      "Epoch 366/2048\n",
      "3s - loss: 15.4976 - val_loss: 15.4295\n",
      "Epoch 367/2048\n",
      "3s - loss: 15.3313 - val_loss: 15.2644\n",
      "Epoch 368/2048\n",
      "3s - loss: 15.1659 - val_loss: 15.0993\n",
      "Epoch 369/2048\n",
      "4s - loss: 15.0055 - val_loss: 14.9358\n",
      "Epoch 370/2048\n",
      "4s - loss: 14.8387 - val_loss: 14.7717\n",
      "Epoch 371/2048\n",
      "4s - loss: 14.6738 - val_loss: 14.6078\n",
      "Epoch 372/2048\n",
      "3s - loss: 14.5130 - val_loss: 14.4450\n",
      "Epoch 373/2048\n",
      "3s - loss: 14.3474 - val_loss: 14.2816\n",
      "Epoch 374/2048\n",
      "4s - loss: 14.1903 - val_loss: 14.1198\n",
      "Epoch 375/2048\n",
      "3s - loss: 14.0224 - val_loss: 13.9568\n",
      "Epoch 376/2048\n",
      "4s - loss: 13.8618 - val_loss: 13.7952\n",
      "Epoch 377/2048\n",
      "4s - loss: 13.7029 - val_loss: 13.6344\n",
      "Epoch 378/2048\n",
      "5s - loss: 13.5482 - val_loss: 13.4822\n",
      "Epoch 379/2048\n",
      "8s - loss: 13.3860 - val_loss: 13.3177\n",
      "Epoch 380/2048\n",
      "4s - loss: 13.2215 - val_loss: 13.1562\n",
      "Epoch 381/2048\n",
      "4s - loss: 13.0599 - val_loss: 12.9958\n",
      "Epoch 382/2048\n",
      "4s - loss: 12.9003 - val_loss: 12.8368\n",
      "Epoch 383/2048\n",
      "4s - loss: 12.7416 - val_loss: 12.6778\n",
      "Epoch 384/2048\n",
      "4s - loss: 12.5879 - val_loss: 12.5207\n",
      "Epoch 385/2048\n",
      "4s - loss: 12.4271 - val_loss: 12.3630\n",
      "Epoch 386/2048\n",
      "3s - loss: 12.2695 - val_loss: 12.2064\n",
      "Epoch 387/2048\n",
      "4s - loss: 12.1130 - val_loss: 12.0502\n",
      "Epoch 388/2048\n",
      "4s - loss: 11.9564 - val_loss: 11.8946\n",
      "Epoch 389/2048\n",
      "4s - loss: 11.8012 - val_loss: 11.7398\n",
      "Epoch 390/2048\n",
      "5s - loss: 11.6481 - val_loss: 11.5860\n",
      "Epoch 391/2048\n",
      "3s - loss: 11.4930 - val_loss: 11.4325\n",
      "Epoch 392/2048\n",
      "4s - loss: 11.3395 - val_loss: 11.2799\n",
      "Epoch 393/2048\n",
      "6s - loss: 11.1920 - val_loss: 11.1286\n",
      "Epoch 394/2048\n",
      "4s - loss: 11.0366 - val_loss: 10.9766\n",
      "Epoch 395/2048\n",
      "3s - loss: 10.8850 - val_loss: 10.8262\n",
      "Epoch 396/2048\n",
      "4s - loss: 10.7351 - val_loss: 10.6764\n",
      "Epoch 397/2048\n",
      "4s - loss: 10.5858 - val_loss: 10.5272\n",
      "Epoch 398/2048\n",
      "5s - loss: 10.4375 - val_loss: 10.3789\n",
      "Epoch 399/2048\n",
      "4s - loss: 10.2892 - val_loss: 10.2316\n",
      "Epoch 400/2048\n",
      "4s - loss: 10.1436 - val_loss: 10.0872\n",
      "Epoch 401/2048\n",
      "4s - loss: 9.9972 - val_loss: 9.9401\n",
      "Epoch 402/2048\n",
      "3s - loss: 9.8512 - val_loss: 9.7946\n",
      "Epoch 403/2048\n",
      "3s - loss: 9.7070 - val_loss: 9.6518\n",
      "Epoch 404/2048\n",
      "4s - loss: 9.5652 - val_loss: 9.5079\n",
      "Epoch 405/2048\n",
      "3s - loss: 9.4223 - val_loss: 9.3675\n",
      "Epoch 406/2048\n",
      "4s - loss: 9.2797 - val_loss: 9.2248\n",
      "Epoch 407/2048\n",
      "4s - loss: 9.1378 - val_loss: 9.0839\n",
      "Epoch 408/2048\n",
      "4s - loss: 8.9986 - val_loss: 8.9452\n",
      "Epoch 409/2048\n",
      "4s - loss: 8.8594 - val_loss: 8.8068\n",
      "Epoch 410/2048\n",
      "3s - loss: 8.7210 - val_loss: 8.6688\n",
      "Epoch 411/2048\n",
      "6s - loss: 8.5844 - val_loss: 8.5327\n",
      "Epoch 412/2048\n",
      "3s - loss: 8.4485 - val_loss: 8.3975\n",
      "Epoch 413/2048\n",
      "4s - loss: 8.3141 - val_loss: 8.2630\n",
      "Epoch 414/2048\n",
      "4s - loss: 8.1802 - val_loss: 8.1298\n",
      "Epoch 415/2048\n",
      "4s - loss: 8.0478 - val_loss: 7.9982\n",
      "Epoch 416/2048\n",
      "15s - loss: 7.9162 - val_loss: 7.8786\n",
      "Epoch 417/2048\n",
      "4s - loss: 7.7881 - val_loss: 7.7392\n",
      "Epoch 418/2048\n",
      "6s - loss: 7.6603 - val_loss: 7.6119\n",
      "Epoch 419/2048\n",
      "4s - loss: 7.5302 - val_loss: 7.4821\n",
      "Epoch 420/2048\n",
      "4s - loss: 7.4024 - val_loss: 7.3551\n",
      "Epoch 421/2048\n",
      "4s - loss: 7.2751 - val_loss: 7.2291\n",
      "Epoch 422/2048\n",
      "4s - loss: 7.1503 - val_loss: 7.1049\n",
      "Epoch 423/2048\n",
      "4s - loss: 7.0262 - val_loss: 6.9892\n",
      "Epoch 424/2048\n",
      "4s - loss: 6.9042 - val_loss: 6.8597\n",
      "Epoch 425/2048\n",
      "4s - loss: 6.7825 - val_loss: 6.7390\n",
      "Epoch 426/2048\n",
      "4s - loss: 6.6624 - val_loss: 6.6200\n",
      "Epoch 427/2048\n",
      "4s - loss: 6.5439 - val_loss: 6.5027\n",
      "Epoch 428/2048\n",
      "4s - loss: 6.4284 - val_loss: 6.3857\n",
      "Epoch 429/2048\n",
      "6s - loss: 6.3106 - val_loss: 6.2692\n",
      "Epoch 430/2048\n",
      "4s - loss: 6.1949 - val_loss: 6.1544\n",
      "Epoch 431/2048\n",
      "4s - loss: 6.0803 - val_loss: 6.0406\n",
      "Epoch 432/2048\n",
      "9s - loss: 5.9678 - val_loss: 5.9289\n",
      "Epoch 433/2048\n",
      "10s - loss: 5.8567 - val_loss: 5.8193\n",
      "Epoch 434/2048\n",
      "16s - loss: 5.7470 - val_loss: 5.7090\n",
      "Epoch 435/2048\n",
      "4s - loss: 5.6377 - val_loss: 5.6070\n",
      "Epoch 436/2048\n",
      "4s - loss: 5.5308 - val_loss: 5.4936\n",
      "Epoch 437/2048\n",
      "5s - loss: 5.4232 - val_loss: 5.3878\n",
      "Epoch 438/2048\n",
      "3s - loss: 5.3180 - val_loss: 5.2838\n",
      "Epoch 439/2048\n",
      "4s - loss: 5.2150 - val_loss: 5.1814\n",
      "Epoch 440/2048\n",
      "4s - loss: 5.1164 - val_loss: 5.0871\n",
      "Epoch 441/2048\n",
      "2s - loss: 5.0062 - val_loss: 5.0988\n",
      "Epoch 442/2048\n",
      "4s - loss: 5.0480 - val_loss: 4.9827\n",
      "Epoch 443/2048\n",
      "5s - loss: 4.9020 - val_loss: 4.8577\n",
      "Epoch 444/2048\n",
      "4s - loss: 4.7855 - val_loss: 4.7474\n",
      "Epoch 445/2048\n",
      "3s - loss: 4.6785 - val_loss: 4.6442\n",
      "Epoch 446/2048\n",
      "4s - loss: 4.5775 - val_loss: 4.5452\n",
      "Epoch 447/2048\n",
      "3s - loss: 4.4800 - val_loss: 4.4493\n",
      "Epoch 448/2048\n",
      "6s - loss: 4.3854 - val_loss: 4.3561\n",
      "Epoch 449/2048\n",
      "4s - loss: 4.2931 - val_loss: 4.2653\n",
      "Epoch 450/2048\n",
      "4s - loss: 4.2030 - val_loss: 4.1758\n",
      "Epoch 451/2048\n",
      "5s - loss: 4.1146 - val_loss: 4.0885\n",
      "Epoch 452/2048\n",
      "4s - loss: 4.0278 - val_loss: 4.0027\n",
      "Epoch 453/2048\n",
      "3s - loss: 3.9431 - val_loss: 3.9188\n",
      "Epoch 454/2048\n",
      "4s - loss: 3.8592 - val_loss: 3.8361\n",
      "Epoch 455/2048\n",
      "4s - loss: 3.7772 - val_loss: 3.7546\n",
      "Epoch 456/2048\n",
      "4s - loss: 3.6961 - val_loss: 3.6790\n",
      "Epoch 457/2048\n",
      "4s - loss: 3.6175 - val_loss: 3.5960\n",
      "Epoch 458/2048\n",
      "4s - loss: 3.5279 - val_loss: 3.5386\n",
      "Epoch 459/2048\n",
      "4s - loss: 3.4848 - val_loss: 3.4874\n",
      "Epoch 460/2048\n",
      "4s - loss: 3.4025 - val_loss: 3.4596\n",
      "Epoch 461/2048\n",
      "4s - loss: 3.3335 - val_loss: 3.3305\n",
      "Epoch 462/2048\n",
      "3s - loss: 3.2446 - val_loss: 3.2598\n",
      "Epoch 463/2048\n",
      "4s - loss: 3.1564 - val_loss: 3.1769\n",
      "Epoch 464/2048\n",
      "4s - loss: 3.0654 - val_loss: 3.1118\n",
      "Epoch 465/2048\n",
      "4s - loss: 3.0268 - val_loss: 3.0654\n",
      "Epoch 466/2048\n",
      "4s - loss: 2.9591 - val_loss: 2.9878\n",
      "Epoch 467/2048\n",
      "3s - loss: 2.8693 - val_loss: 2.9207\n",
      "Epoch 468/2048\n",
      "4s - loss: 2.7895 - val_loss: 2.8436\n",
      "Epoch 469/2048\n",
      "2s - loss: 2.8567 - val_loss: 2.8486\n",
      "Epoch 470/2048\n",
      "3s - loss: 2.7952 - val_loss: 2.7729\n",
      "Epoch 471/2048\n",
      "3s - loss: 2.7200 - val_loss: 2.6985\n",
      "Epoch 472/2048\n",
      "1s - loss: 2.6573 - val_loss: 2.7010\n",
      "Epoch 473/2048\n",
      "4s - loss: 2.6433 - val_loss: 2.6201\n",
      "Epoch 474/2048\n",
      "4s - loss: 2.5718 - val_loss: 2.5533\n",
      "Epoch 475/2048\n",
      "5s - loss: 2.5043 - val_loss: 2.4925\n",
      "Epoch 476/2048\n",
      "4s - loss: 2.4290 - val_loss: 2.4336\n",
      "Epoch 477/2048\n",
      "3s - loss: 2.3508 - val_loss: 2.3688\n",
      "Epoch 478/2048\n",
      "5s - loss: 2.2840 - val_loss: 2.3311\n",
      "Epoch 479/2048\n",
      "4s - loss: 2.2302 - val_loss: 2.2994\n",
      "Epoch 480/2048\n",
      "4s - loss: 2.1733 - val_loss: 2.2377\n",
      "Epoch 481/2048\n",
      "3s - loss: 2.1351 - val_loss: 2.1709\n",
      "Epoch 482/2048\n",
      "3s - loss: 2.0610 - val_loss: 2.1163\n",
      "Epoch 483/2048\n",
      "4s - loss: 2.0097 - val_loss: 2.0742\n",
      "Epoch 484/2048\n",
      "2s - loss: 2.0178 - val_loss: 2.1150\n",
      "Epoch 485/2048\n",
      "3s - loss: 2.0284 - val_loss: 2.0471\n",
      "Epoch 486/2048\n",
      "3s - loss: 1.9558 - val_loss: 1.9918\n",
      "Epoch 487/2048\n",
      "3s - loss: 1.8968 - val_loss: 1.9448\n",
      "Epoch 488/2048\n",
      "3s - loss: 1.8414 - val_loss: 1.8943\n",
      "Epoch 489/2048\n",
      "7s - loss: 1.7924 - val_loss: 1.8525\n",
      "Epoch 490/2048\n",
      "15s - loss: 1.7459 - val_loss: 1.8076\n",
      "Epoch 491/2048\n",
      "3s - loss: 1.6983 - val_loss: 1.7908\n",
      "Epoch 492/2048\n",
      "3s - loss: 1.6714 - val_loss: 1.7282\n",
      "Epoch 493/2048\n",
      "3s - loss: 1.6192 - val_loss: 1.6897\n",
      "Epoch 494/2048\n",
      "4s - loss: 1.5896 - val_loss: 1.6591\n",
      "Epoch 495/2048\n",
      "1s - loss: 1.5447 - val_loss: 1.6727\n",
      "Epoch 496/2048\n",
      "3s - loss: 1.5468 - val_loss: 1.6303\n",
      "Epoch 497/2048\n",
      "3s - loss: 1.5152 - val_loss: 1.5736\n",
      "Epoch 498/2048\n",
      "3s - loss: 1.4594 - val_loss: 1.5473\n",
      "Epoch 499/2048\n",
      "4s - loss: 1.4303 - val_loss: 1.5119\n",
      "Epoch 500/2048\n",
      "3s - loss: 1.3864 - val_loss: 1.4762\n",
      "Epoch 501/2048\n",
      "4s - loss: 1.3865 - val_loss: 1.4438\n",
      "Epoch 502/2048\n",
      "3s - loss: 1.3283 - val_loss: 1.3940\n",
      "Epoch 503/2048\n",
      "2s - loss: 1.4368 - val_loss: 1.4614\n",
      "Epoch 504/2048\n",
      "2s - loss: 1.3586 - val_loss: 1.4043\n",
      "Epoch 505/2048\n",
      "5s - loss: 1.3074 - val_loss: 1.3730\n",
      "Epoch 506/2048\n",
      "12s - loss: 1.2561 - val_loss: 1.3319\n",
      "Epoch 507/2048\n",
      "4s - loss: 1.2058 - val_loss: 1.2855\n",
      "Epoch 508/2048\n",
      "5s - loss: 1.1807 - val_loss: 1.2624\n",
      "Epoch 509/2048\n",
      "4s - loss: 1.1396 - val_loss: 1.2385\n",
      "Epoch 510/2048\n",
      "4s - loss: 1.1193 - val_loss: 1.2032\n",
      "Epoch 511/2048\n",
      "6s - loss: 1.1157 - val_loss: 1.1936\n",
      "Epoch 512/2048\n",
      "3s - loss: 1.1029 - val_loss: 1.1829\n",
      "Epoch 513/2048\n",
      "2s - loss: 1.0590 - val_loss: 1.2497\n",
      "Epoch 514/2048\n",
      "4s - loss: 1.0842 - val_loss: 1.1288\n",
      "Epoch 515/2048\n",
      "2s - loss: 1.0498 - val_loss: 1.2012\n",
      "Epoch 516/2048\n",
      "4s - loss: 1.0741 - val_loss: 1.1101\n",
      "Epoch 517/2048\n",
      "4s - loss: 0.9950 - val_loss: 1.0866\n",
      "Epoch 518/2048\n",
      "4s - loss: 0.9843 - val_loss: 1.0571\n",
      "Epoch 519/2048\n",
      "2s - loss: 0.9579 - val_loss: 1.1245\n",
      "Epoch 520/2048\n",
      "5s - loss: 0.9676 - val_loss: 1.0246\n",
      "Epoch 521/2048\n",
      "2s - loss: 0.9358 - val_loss: 1.0375\n",
      "Epoch 522/2048\n",
      "3s - loss: 0.9162 - val_loss: 0.9999\n",
      "Epoch 523/2048\n",
      "1s - loss: 0.9698 - val_loss: 1.1119\n",
      "Epoch 524/2048\n",
      "2s - loss: 1.0022 - val_loss: 1.0519\n",
      "Epoch 525/2048\n",
      "2s - loss: 0.9186 - val_loss: 1.0412\n",
      "Epoch 526/2048\n",
      "2s - loss: 0.8942 - val_loss: 1.8468\n",
      "Epoch 527/2048\n",
      "2s - loss: 1.0997 - val_loss: 1.1109\n",
      "Epoch 528/2048\n",
      "2s - loss: 1.0167 - val_loss: 1.0729\n",
      "Epoch 529/2048\n",
      "2s - loss: 0.9698 - val_loss: 1.0376\n",
      "Epoch 530/2048\n",
      "1s - loss: 0.9482 - val_loss: 1.0305\n",
      "Epoch 531/2048\n",
      "1s - loss: 0.9128 - val_loss: 1.0222\n",
      "Epoch 532/2048\n",
      "3s - loss: 0.8741 - val_loss: 0.9477\n",
      "Epoch 533/2048\n",
      "4s - loss: 0.8453 - val_loss: 0.9343\n",
      "Epoch 534/2048\n",
      "2s - loss: 0.8292 - val_loss: 0.9930\n",
      "Epoch 535/2048\n",
      "3s - loss: 0.8276 - val_loss: 0.9037\n",
      "Epoch 536/2048\n",
      "2s - loss: 0.8378 - val_loss: 0.9365\n",
      "Epoch 537/2048\n",
      "3s - loss: 0.7993 - val_loss: 0.8865\n",
      "Epoch 538/2048\n",
      "5s - loss: 0.7970 - val_loss: 0.8767\n",
      "Epoch 539/2048\n",
      "2s - loss: 0.7896 - val_loss: 0.9065\n",
      "Epoch 540/2048\n",
      "4s - loss: 0.7765 - val_loss: 0.8555\n",
      "Epoch 541/2048\n",
      "4s - loss: 0.7675 - val_loss: 0.8477\n",
      "Epoch 542/2048\n",
      "2s - loss: 0.7556 - val_loss: 0.8750\n",
      "Epoch 543/2048\n",
      "3s - loss: 0.7506 - val_loss: 0.8328\n",
      "Epoch 544/2048\n",
      "3s - loss: 0.7285 - val_loss: 0.8273\n",
      "Epoch 545/2048\n",
      "4s - loss: 0.7222 - val_loss: 0.8143\n",
      "Epoch 546/2048\n",
      "2s - loss: 0.7155 - val_loss: 0.8871\n",
      "Epoch 547/2048\n",
      "4s - loss: 0.7209 - val_loss: 0.8059\n",
      "Epoch 548/2048\n",
      "4s - loss: 0.7039 - val_loss: 0.7902\n",
      "Epoch 549/2048\n",
      "2s - loss: 0.6879 - val_loss: 0.8112\n",
      "Epoch 550/2048\n",
      "3s - loss: 0.6912 - val_loss: 0.7898\n",
      "Epoch 551/2048\n",
      "1s - loss: 0.7142 - val_loss: 0.8418\n",
      "Epoch 552/2048\n",
      "1s - loss: 0.7119 - val_loss: 0.7941\n",
      "Epoch 553/2048\n",
      "3s - loss: 0.6816 - val_loss: 0.7687\n",
      "Epoch 554/2048\n",
      "2s - loss: 0.6707 - val_loss: 0.7732\n",
      "Epoch 555/2048\n",
      "2s - loss: 0.6780 - val_loss: 0.7832\n",
      "Epoch 556/2048\n",
      "2s - loss: 0.7653 - val_loss: 0.8547\n",
      "Epoch 557/2048\n",
      "2s - loss: 0.7428 - val_loss: 0.8119\n",
      "Epoch 558/2048\n",
      "1s - loss: 0.6983 - val_loss: 0.7935\n",
      "Epoch 559/2048\n",
      "2s - loss: 0.6811 - val_loss: 0.7787\n",
      "Epoch 560/2048\n",
      "2s - loss: 0.6692 - val_loss: 0.8779\n",
      "Epoch 561/2048\n",
      "4s - loss: 0.6789 - val_loss: 0.7485\n",
      "Epoch 562/2048\n",
      "3s - loss: 0.6495 - val_loss: 0.7464\n",
      "Epoch 563/2048\n",
      "3s - loss: 0.6427 - val_loss: 0.7324\n",
      "Epoch 564/2048\n",
      "2s - loss: 0.6950 - val_loss: 0.8829\n",
      "Epoch 565/2048\n",
      "2s - loss: 0.7892 - val_loss: 0.8154\n",
      "Epoch 566/2048\n",
      "2s - loss: 0.6932 - val_loss: 0.7656\n",
      "Epoch 567/2048\n",
      "2s - loss: 0.6661 - val_loss: 0.7668\n",
      "Epoch 568/2048\n",
      "2s - loss: 0.6515 - val_loss: 0.7345\n",
      "Epoch 569/2048\n",
      "2s - loss: 0.6427 - val_loss: 0.7663\n",
      "Epoch 570/2048\n",
      "4s - loss: 0.6356 - val_loss: 0.7202\n",
      "Epoch 571/2048\n",
      "4s - loss: 0.6290 - val_loss: 0.7198\n",
      "Epoch 572/2048\n",
      "4s - loss: 0.6256 - val_loss: 0.7110\n",
      "Epoch 573/2048\n",
      "2s - loss: 0.6325 - val_loss: 0.7118\n",
      "Epoch 574/2048\n",
      "1s - loss: 0.6433 - val_loss: 0.7275\n",
      "Epoch 575/2048\n",
      "1s - loss: 0.6208 - val_loss: 0.7150\n",
      "Epoch 576/2048\n",
      "3s - loss: 0.6169 - val_loss: 0.6960\n",
      "Epoch 577/2048\n",
      "2s - loss: 0.6130 - val_loss: 0.7080\n",
      "Epoch 578/2048\n",
      "5s - loss: 0.6032 - val_loss: 0.6890\n",
      "Epoch 579/2048\n",
      "4s - loss: 0.5959 - val_loss: 0.6814\n",
      "Epoch 580/2048\n",
      "2s - loss: 0.5992 - val_loss: 0.6949\n",
      "Epoch 581/2048\n",
      "1s - loss: 0.6042 - val_loss: 0.7240\n",
      "Epoch 582/2048\n",
      "2s - loss: 0.6034 - val_loss: 0.6832\n",
      "Epoch 583/2048\n",
      "2s - loss: 0.6200 - val_loss: 0.7635\n",
      "Epoch 584/2048\n",
      "2s - loss: 0.6305 - val_loss: 0.6894\n",
      "Epoch 585/2048\n",
      "4s - loss: 0.5919 - val_loss: 0.6746\n",
      "Epoch 586/2048\n",
      "2s - loss: 0.5895 - val_loss: 0.7080\n",
      "Epoch 587/2048\n",
      "4s - loss: 0.5910 - val_loss: 0.6699\n",
      "Epoch 588/2048\n",
      "2s - loss: 0.5929 - val_loss: 0.6889\n",
      "Epoch 589/2048\n",
      "4s - loss: 0.5851 - val_loss: 0.6633\n",
      "Epoch 590/2048\n",
      "2s - loss: 0.5890 - val_loss: 0.7054\n",
      "Epoch 591/2048\n",
      "4s - loss: 0.5854 - val_loss: 0.6581\n",
      "Epoch 592/2048\n",
      "2s - loss: 0.5740 - val_loss: 0.6619\n",
      "Epoch 593/2048\n",
      "4s - loss: 0.5715 - val_loss: 0.6545\n",
      "Epoch 594/2048\n",
      "2s - loss: 0.5671 - val_loss: 0.6675\n",
      "Epoch 595/2048\n",
      "2s - loss: 0.5710 - val_loss: 0.6631\n",
      "Epoch 596/2048\n",
      "2s - loss: 0.5806 - val_loss: 0.6672\n",
      "Epoch 597/2048\n",
      "2s - loss: 0.5799 - val_loss: 0.6649\n",
      "Epoch 598/2048\n",
      "2s - loss: 0.5666 - val_loss: 0.7365\n",
      "Epoch 599/2048\n",
      "2s - loss: 0.7381 - val_loss: 0.7523\n",
      "Epoch 600/2048\n",
      "2s - loss: 0.6261 - val_loss: 0.7032\n",
      "Epoch 601/2048\n",
      "2s - loss: 0.5953 - val_loss: 0.6709\n",
      "Epoch 602/2048\n",
      "2s - loss: 0.5756 - val_loss: 0.6690\n",
      "Epoch 603/2048\n",
      "2s - loss: 0.5746 - val_loss: 0.6628\n",
      "Epoch 604/2048\n",
      "2s - loss: 0.5649 - val_loss: 0.7239\n",
      "Epoch 605/2048\n",
      "3s - loss: 0.5800 - val_loss: 0.6542\n",
      "Epoch 606/2048\n",
      "2s - loss: 0.6210 - val_loss: 0.7043\n",
      "Epoch 607/2048\n",
      "2s - loss: 0.5911 - val_loss: 0.6696\n",
      "Epoch 608/2048\n",
      "2s - loss: 0.5702 - val_loss: 0.6906\n",
      "Epoch 609/2048\n",
      "2s - loss: 0.5704 - val_loss: 0.6571\n",
      "Epoch 610/2048\n",
      "4s - loss: 0.5683 - val_loss: 0.6537\n",
      "Epoch 611/2048\n",
      "2s - loss: 0.5637 - val_loss: 0.6975\n",
      "Epoch 612/2048\n",
      "3s - loss: 0.5691 - val_loss: 0.6503\n",
      "Epoch 613/2048\n",
      "2s - loss: 0.5837 - val_loss: 0.7119\n",
      "Epoch 614/2048\n",
      "4s - loss: 0.5788 - val_loss: 0.6446\n",
      "Epoch 615/2048\n",
      "2s - loss: 0.5606 - val_loss: 0.6595\n",
      "Epoch 616/2048\n",
      "2s - loss: 0.5579 - val_loss: 0.6486\n",
      "Epoch 617/2048\n",
      "4s - loss: 0.5541 - val_loss: 0.6410\n",
      "Epoch 618/2048\n",
      "2s - loss: 0.5495 - val_loss: 0.6429\n",
      "Epoch 619/2048\n",
      "4s - loss: 0.5483 - val_loss: 0.6282\n",
      "Epoch 620/2048\n",
      "2s - loss: 0.5488 - val_loss: 0.7160\n",
      "Epoch 621/2048\n",
      "2s - loss: 0.5731 - val_loss: 0.6418\n",
      "Epoch 622/2048\n",
      "2s - loss: 0.5473 - val_loss: 0.6537\n",
      "Epoch 623/2048\n",
      "4s - loss: 0.5455 - val_loss: 0.6253\n",
      "Epoch 624/2048\n",
      "2s - loss: 0.5450 - val_loss: 0.6318\n",
      "Epoch 625/2048\n",
      "2s - loss: 0.5585 - val_loss: 0.6462\n",
      "Epoch 626/2048\n",
      "2s - loss: 0.5437 - val_loss: 0.6283\n",
      "Epoch 627/2048\n",
      "2s - loss: 0.5370 - val_loss: 0.6499\n",
      "Epoch 628/2048\n",
      "2s - loss: 0.5652 - val_loss: 0.6534\n",
      "Epoch 629/2048\n",
      "2s - loss: 0.5484 - val_loss: 0.6271\n",
      "Epoch 630/2048\n",
      "2s - loss: 0.5510 - val_loss: 0.6352\n",
      "Epoch 631/2048\n",
      "4s - loss: 0.5417 - val_loss: 0.6229\n",
      "Epoch 632/2048\n",
      "4s - loss: 0.5385 - val_loss: 0.6213\n",
      "Epoch 633/2048\n",
      "1s - loss: 0.5371 - val_loss: 0.6244\n",
      "Epoch 634/2048\n",
      "1s - loss: 0.5452 - val_loss: 0.6311\n",
      "Epoch 635/2048\n",
      "2s - loss: 0.5377 - val_loss: 0.6350\n",
      "Epoch 636/2048\n",
      "9s - loss: 0.5348 - val_loss: 0.6180\n",
      "Epoch 637/2048\n",
      "2s - loss: 0.5377 - val_loss: 0.6259\n",
      "Epoch 638/2048\n",
      "2s - loss: 0.5408 - val_loss: 0.6677\n",
      "Epoch 639/2048\n",
      "2s - loss: 0.5537 - val_loss: 0.6229\n",
      "Epoch 640/2048\n",
      "3s - loss: 0.5318 - val_loss: 0.6141\n",
      "Epoch 641/2048\n",
      "2s - loss: 0.5340 - val_loss: 0.6332\n",
      "Epoch 642/2048\n",
      "1s - loss: 0.5354 - val_loss: 0.6185\n",
      "Epoch 643/2048\n",
      "3s - loss: 0.5345 - val_loss: 0.6104\n",
      "Epoch 644/2048\n",
      "2s - loss: 0.5254 - val_loss: 0.6181\n",
      "Epoch 645/2048\n",
      "2s - loss: 0.5329 - val_loss: 0.6165\n",
      "Epoch 646/2048\n",
      "4s - loss: 0.5260 - val_loss: 0.6103\n",
      "Epoch 647/2048\n",
      "4s - loss: 0.5246 - val_loss: 0.6073\n",
      "Epoch 648/2048\n",
      "2s - loss: 0.5213 - val_loss: 0.6107\n",
      "Epoch 649/2048\n",
      "1s - loss: 0.5458 - val_loss: 0.6137\n",
      "Epoch 650/2048\n",
      "2s - loss: 0.5344 - val_loss: 0.6378\n",
      "Epoch 651/2048\n",
      "3s - loss: 0.5248 - val_loss: 0.6055\n",
      "Epoch 652/2048\n",
      "2s - loss: 0.5199 - val_loss: 0.6120\n",
      "Epoch 653/2048\n",
      "4s - loss: 0.5184 - val_loss: 0.5999\n",
      "Epoch 654/2048\n",
      "2s - loss: 0.5258 - val_loss: 0.6348\n",
      "Epoch 655/2048\n",
      "2s - loss: 0.5454 - val_loss: 0.6200\n",
      "Epoch 656/2048\n",
      "2s - loss: 0.5314 - val_loss: 0.6106\n",
      "Epoch 657/2048\n",
      "2s - loss: 0.5202 - val_loss: 0.6050\n",
      "Epoch 658/2048\n",
      "2s - loss: 0.5213 - val_loss: 0.6217\n",
      "Epoch 659/2048\n",
      "2s - loss: 0.5266 - val_loss: 0.6139\n",
      "Epoch 660/2048\n",
      "2s - loss: 0.5269 - val_loss: 0.6146\n",
      "Epoch 661/2048\n",
      "2s - loss: 0.5163 - val_loss: 0.6035\n",
      "Epoch 662/2048\n",
      "2s - loss: 0.5161 - val_loss: 0.6051\n",
      "Epoch 663/2048\n",
      "3s - loss: 0.5203 - val_loss: 0.5969\n",
      "Epoch 664/2048\n",
      "2s - loss: 0.5182 - val_loss: 0.6232\n",
      "Epoch 665/2048\n",
      "2s - loss: 0.5199 - val_loss: 0.6018\n",
      "Epoch 666/2048\n",
      "2s - loss: 0.5185 - val_loss: 0.6125\n",
      "Epoch 667/2048\n",
      "2s - loss: 0.5223 - val_loss: 0.6141\n",
      "Epoch 668/2048\n",
      "2s - loss: 0.5160 - val_loss: 0.6381\n",
      "Epoch 669/2048\n",
      "2s - loss: 0.5608 - val_loss: 0.6273\n",
      "Epoch 670/2048\n",
      "1s - loss: 0.5203 - val_loss: 0.6092\n",
      "Epoch 671/2048\n",
      "2s - loss: 0.5156 - val_loss: 0.6009\n",
      "Epoch 672/2048\n",
      "4s - loss: 0.5159 - val_loss: 0.5944\n",
      "Epoch 673/2048\n",
      "2s - loss: 0.5105 - val_loss: 0.5972\n",
      "Epoch 674/2048\n",
      "2s - loss: 0.5132 - val_loss: 0.5963\n",
      "Epoch 675/2048\n",
      "2s - loss: 0.5101 - val_loss: 0.5962\n",
      "Epoch 676/2048\n",
      "2s - loss: 0.5116 - val_loss: 0.6057\n",
      "Epoch 677/2048\n",
      "2s - loss: 0.5187 - val_loss: 0.5977\n",
      "Epoch 678/2048\n",
      "10s - loss: 0.5096 - val_loss: 0.5915\n",
      "Epoch 679/2048\n",
      "2s - loss: 0.5105 - val_loss: 0.6003\n",
      "Epoch 680/2048\n",
      "2s - loss: 0.5192 - val_loss: 0.5937\n",
      "Epoch 681/2048\n",
      "2s - loss: 0.5098 - val_loss: 0.5991\n",
      "Epoch 682/2048\n",
      "8s - loss: 0.5081 - val_loss: 0.5891\n",
      "Epoch 683/2048\n",
      "1s - loss: 0.5434 - val_loss: 0.6241\n",
      "Epoch 684/2048\n",
      "2s - loss: 0.5255 - val_loss: 0.6187\n",
      "Epoch 685/2048\n",
      "2s - loss: 0.5220 - val_loss: 0.6079\n",
      "Epoch 686/2048\n",
      "2s - loss: 0.5177 - val_loss: 0.5963\n",
      "Epoch 687/2048\n",
      "2s - loss: 0.5121 - val_loss: 0.6061\n",
      "Epoch 688/2048\n",
      "2s - loss: 0.5107 - val_loss: 0.6335\n",
      "Epoch 689/2048\n",
      "2s - loss: 0.5132 - val_loss: 0.5907\n",
      "Epoch 690/2048\n",
      "2s - loss: 0.5116 - val_loss: 0.6050\n",
      "Epoch 691/2048\n",
      "2s - loss: 0.5240 - val_loss: 0.5924\n",
      "Epoch 692/2048\n",
      "2s - loss: 0.5086 - val_loss: 0.5924\n",
      "Epoch 693/2048\n",
      "1s - loss: 0.5107 - val_loss: 0.5917\n",
      "Epoch 694/2048\n",
      "1s - loss: 0.5102 - val_loss: 0.6066\n",
      "Epoch 695/2048\n",
      "13s - loss: 0.5065 - val_loss: 0.5873\n",
      "Epoch 696/2048\n",
      "15s - loss: 0.5046 - val_loss: 0.5848\n",
      "Epoch 697/2048\n",
      "2s - loss: 0.5041 - val_loss: 0.6043\n",
      "Epoch 698/2048\n",
      "2s - loss: 0.5054 - val_loss: 0.5869\n",
      "Epoch 699/2048\n",
      "2s - loss: 0.5030 - val_loss: 0.5952\n",
      "Epoch 700/2048\n",
      "13s - loss: 0.5044 - val_loss: 0.5846\n",
      "Epoch 701/2048\n",
      "1s - loss: 0.5015 - val_loss: 0.5919\n",
      "Epoch 702/2048\n",
      "9s - loss: 0.4987 - val_loss: 0.5828\n",
      "Epoch 703/2048\n",
      "1s - loss: 0.5044 - val_loss: 0.5886\n",
      "Epoch 704/2048\n",
      "2s - loss: 0.4991 - val_loss: 0.5834\n",
      "Epoch 705/2048\n",
      "2s - loss: 0.5128 - val_loss: 0.6178\n",
      "Epoch 706/2048\n",
      "2s - loss: 0.5050 - val_loss: 0.6110\n",
      "Epoch 707/2048\n",
      "2s - loss: 0.5106 - val_loss: 0.5903\n",
      "Epoch 708/2048\n",
      "2s - loss: 0.5014 - val_loss: 0.5831\n",
      "Epoch 709/2048\n",
      "2s - loss: 0.5067 - val_loss: 0.5890\n",
      "Epoch 710/2048\n",
      "2s - loss: 0.5037 - val_loss: 0.6275\n",
      "Epoch 711/2048\n",
      "2s - loss: 0.5133 - val_loss: 0.5829\n",
      "Epoch 712/2048\n",
      "2s - loss: 0.5159 - val_loss: 0.6237\n",
      "Epoch 713/2048\n",
      "2s - loss: 0.5176 - val_loss: 0.5910\n",
      "Epoch 714/2048\n",
      "2s - loss: 0.5012 - val_loss: 0.5840\n",
      "Epoch 715/2048\n",
      "1s - loss: 0.4996 - val_loss: 0.5914\n",
      "Epoch 716/2048\n",
      "4s - loss: 0.4991 - val_loss: 0.5808\n",
      "Epoch 717/2048\n",
      "2s - loss: 0.4965 - val_loss: 0.5809\n",
      "Epoch 718/2048\n",
      "2s - loss: 0.4971 - val_loss: 0.5812\n",
      "Epoch 719/2048\n",
      "2s - loss: 0.4995 - val_loss: 0.5832\n",
      "Epoch 720/2048\n",
      "2s - loss: 0.4975 - val_loss: 0.5866\n",
      "Epoch 721/2048\n",
      "8s - loss: 0.5003 - val_loss: 0.5784\n",
      "Epoch 722/2048\n",
      "2s - loss: 0.4986 - val_loss: 0.5811\n",
      "Epoch 723/2048\n",
      "1s - loss: 0.5076 - val_loss: 0.5905\n",
      "Epoch 724/2048\n",
      "2s - loss: 0.4999 - val_loss: 0.5786\n",
      "Epoch 725/2048\n",
      "5s - loss: 0.4945 - val_loss: 0.5740\n",
      "Epoch 726/2048\n",
      "2s - loss: 0.4984 - val_loss: 0.5784\n",
      "Epoch 727/2048\n",
      "2s - loss: 0.5114 - val_loss: 0.6072\n",
      "Epoch 728/2048\n",
      "1s - loss: 0.5057 - val_loss: 0.5840\n",
      "Epoch 729/2048\n",
      "1s - loss: 0.5004 - val_loss: 0.5847\n",
      "Epoch 730/2048\n",
      "2s - loss: 0.4964 - val_loss: 0.5808\n",
      "Epoch 731/2048\n",
      "2s - loss: 0.4942 - val_loss: 0.5803\n",
      "Epoch 732/2048\n",
      "2s - loss: 0.5048 - val_loss: 0.5776\n",
      "Epoch 733/2048\n",
      "1s - loss: 0.4940 - val_loss: 0.6596\n",
      "Epoch 734/2048\n",
      "2s - loss: 0.5142 - val_loss: 0.5797\n",
      "Epoch 735/2048\n",
      "2s - loss: 0.4954 - val_loss: 0.5806\n",
      "Epoch 736/2048\n",
      "2s - loss: 0.4955 - val_loss: 0.5758\n",
      "Epoch 737/2048\n",
      "4s - loss: 0.4952 - val_loss: 0.5737\n",
      "Epoch 738/2048\n",
      "2s - loss: 0.4922 - val_loss: 0.5791\n",
      "Epoch 739/2048\n",
      "2s - loss: 0.4970 - val_loss: 0.5759\n",
      "Epoch 740/2048\n",
      "2s - loss: 0.4984 - val_loss: 0.5787\n",
      "Epoch 741/2048\n",
      "2s - loss: 0.4955 - val_loss: 0.5814\n",
      "Epoch 742/2048\n",
      "2s - loss: 0.4968 - val_loss: 0.5844\n",
      "Epoch 743/2048\n",
      "1s - loss: 0.4925 - val_loss: 0.5751\n",
      "Epoch 744/2048\n",
      "2s - loss: 0.4889 - val_loss: 0.5797\n",
      "Epoch 745/2048\n",
      "4s - loss: 0.4903 - val_loss: 0.5713\n",
      "Epoch 746/2048\n",
      "2s - loss: 0.4911 - val_loss: 0.5729\n",
      "Epoch 747/2048\n",
      "2s - loss: 0.4947 - val_loss: 0.5786\n",
      "Epoch 748/2048\n",
      "2s - loss: 0.4960 - val_loss: 0.5866\n",
      "Epoch 749/2048\n",
      "2s - loss: 0.4950 - val_loss: 0.5735\n",
      "Epoch 750/2048\n",
      "3s - loss: 0.4895 - val_loss: 0.5711\n",
      "Epoch 751/2048\n",
      "2s - loss: 0.4926 - val_loss: 0.5749\n",
      "Epoch 752/2048\n",
      "4s - loss: 0.4915 - val_loss: 0.5698\n",
      "Epoch 753/2048\n",
      "2s - loss: 0.4991 - val_loss: 0.5799\n",
      "Epoch 754/2048\n",
      "2s - loss: 0.4883 - val_loss: 0.5710\n",
      "Epoch 755/2048\n",
      "2s - loss: 0.4917 - val_loss: 0.5744\n",
      "Epoch 756/2048\n",
      "5s - loss: 0.4903 - val_loss: 0.5681\n",
      "Epoch 757/2048\n",
      "2s - loss: 0.4890 - val_loss: 0.5777\n",
      "Epoch 758/2048\n",
      "2s - loss: 0.4999 - val_loss: 0.5718\n",
      "Epoch 759/2048\n",
      "3s - loss: 0.4855 - val_loss: 0.5661\n",
      "Epoch 760/2048\n",
      "4s - loss: 0.4836 - val_loss: 0.5650\n",
      "Epoch 761/2048\n",
      "2s - loss: 0.4874 - val_loss: 0.5659\n",
      "Epoch 762/2048\n",
      "2s - loss: 0.4872 - val_loss: 0.5665\n",
      "Epoch 763/2048\n",
      "2s - loss: 0.4860 - val_loss: 0.5727\n",
      "Epoch 764/2048\n",
      "2s - loss: 0.4888 - val_loss: 0.5684\n",
      "Epoch 765/2048\n",
      "2s - loss: 0.4951 - val_loss: 0.5690\n",
      "Epoch 766/2048\n",
      "2s - loss: 0.4883 - val_loss: 0.5795\n",
      "Epoch 767/2048\n",
      "2s - loss: 0.5002 - val_loss: 0.5763\n",
      "Epoch 768/2048\n",
      "2s - loss: 0.4865 - val_loss: 0.5710\n",
      "Epoch 769/2048\n",
      "2s - loss: 0.4887 - val_loss: 0.5728\n",
      "Epoch 770/2048\n",
      "1s - loss: 0.4942 - val_loss: 0.5782\n",
      "Epoch 771/2048\n",
      "1s - loss: 0.4979 - val_loss: 0.5908\n",
      "Epoch 772/2048\n",
      "2s - loss: 0.5038 - val_loss: 0.5896\n",
      "Epoch 773/2048\n",
      "2s - loss: 0.4914 - val_loss: 0.5685\n",
      "Epoch 774/2048\n",
      "2s - loss: 0.5194 - val_loss: 0.6026\n",
      "Epoch 775/2048\n",
      "2s - loss: 0.5063 - val_loss: 0.5934\n",
      "Epoch 776/2048\n",
      "2s - loss: 0.5081 - val_loss: 0.5845\n",
      "Epoch 777/2048\n",
      "2s - loss: 0.4972 - val_loss: 0.5776\n",
      "Epoch 778/2048\n",
      "2s - loss: 0.4887 - val_loss: 0.5711\n",
      "Epoch 779/2048\n",
      "2s - loss: 0.4906 - val_loss: 0.5650\n",
      "Epoch 780/2048\n",
      "2s - loss: 0.4856 - val_loss: 0.5655\n",
      "Epoch 781/2048\n",
      "2s - loss: 0.4850 - val_loss: 0.5676\n",
      "Epoch 782/2048\n",
      "2s - loss: 0.4919 - val_loss: 0.5743\n",
      "Epoch 783/2048\n",
      "2s - loss: 0.4839 - val_loss: 0.5672\n",
      "Epoch 784/2048\n",
      "2s - loss: 0.4841 - val_loss: 0.5693\n",
      "Epoch 785/2048\n",
      "2s - loss: 0.4839 - val_loss: 0.5661\n",
      "Epoch 786/2048\n",
      "1s - loss: 0.4902 - val_loss: 0.5662\n",
      "Epoch 787/2048\n",
      "3s - loss: 0.4898 - val_loss: 0.5633\n",
      "Epoch 788/2048\n",
      "2s - loss: 0.4851 - val_loss: 0.5634\n",
      "Epoch 789/2048\n",
      "2s - loss: 0.4816 - val_loss: 0.5681\n",
      "Epoch 790/2048\n",
      "2s - loss: 0.4851 - val_loss: 0.5673\n",
      "Epoch 791/2048\n",
      "2s - loss: 0.4897 - val_loss: 0.5704\n",
      "Epoch 792/2048\n",
      "2s - loss: 0.4827 - val_loss: 0.5781\n",
      "Epoch 793/2048\n",
      "3s - loss: 0.4834 - val_loss: 0.5626\n",
      "Epoch 794/2048\n",
      "3s - loss: 0.4840 - val_loss: 0.5622\n",
      "Epoch 795/2048\n",
      "5s - loss: 0.4831 - val_loss: 0.5620\n",
      "Epoch 796/2048\n",
      "7s - loss: 0.4818 - val_loss: 0.5619\n",
      "Epoch 797/2048\n",
      "5s - loss: 0.4833 - val_loss: 0.5616\n",
      "Epoch 798/2048\n",
      "4s - loss: 0.4800 - val_loss: 0.5604\n",
      "Epoch 799/2048\n",
      "2s - loss: 0.4831 - val_loss: 0.5651\n",
      "Epoch 800/2048\n",
      "4s - loss: 0.4855 - val_loss: 0.5603\n",
      "Epoch 801/2048\n",
      "2s - loss: 0.4835 - val_loss: 0.5949\n",
      "Epoch 802/2048\n",
      "2s - loss: 0.4930 - val_loss: 0.5832\n",
      "Epoch 803/2048\n",
      "2s - loss: 0.4868 - val_loss: 0.5615\n",
      "Epoch 804/2048\n",
      "4s - loss: 0.4815 - val_loss: 0.5598\n",
      "Epoch 805/2048\n",
      "2s - loss: 0.4800 - val_loss: 0.5600\n",
      "Epoch 806/2048\n",
      "2s - loss: 0.4799 - val_loss: 0.5653\n",
      "Epoch 807/2048\n",
      "1s - loss: 0.4804 - val_loss: 0.5671\n",
      "Epoch 808/2048\n",
      "1s - loss: 0.5069 - val_loss: 0.5758\n",
      "Epoch 809/2048\n",
      "1s - loss: 0.4928 - val_loss: 0.5679\n",
      "Epoch 810/2048\n",
      "2s - loss: 0.4837 - val_loss: 0.5603\n",
      "Epoch 811/2048\n",
      "2s - loss: 0.4822 - val_loss: 0.5647\n",
      "Epoch 812/2048\n",
      "2s - loss: 0.4832 - val_loss: 0.5637\n",
      "Epoch 813/2048\n",
      "2s - loss: 0.4805 - val_loss: 0.5687\n",
      "Epoch 814/2048\n",
      "4s - loss: 0.4811 - val_loss: 0.5583\n",
      "Epoch 815/2048\n",
      "2s - loss: 0.4780 - val_loss: 0.5612\n",
      "Epoch 816/2048\n",
      "2s - loss: 0.4820 - val_loss: 0.5606\n",
      "Epoch 817/2048\n",
      "2s - loss: 0.4810 - val_loss: 0.5784\n",
      "Epoch 818/2048\n",
      "2s - loss: 0.4860 - val_loss: 0.5649\n",
      "Epoch 819/2048\n",
      "2s - loss: 0.4774 - val_loss: 0.5694\n",
      "Epoch 820/2048\n",
      "2s - loss: 0.4841 - val_loss: 0.5675\n",
      "Epoch 821/2048\n",
      "2s - loss: 0.4808 - val_loss: 0.5684\n",
      "Epoch 822/2048\n",
      "2s - loss: 0.4815 - val_loss: 0.5612\n",
      "Epoch 823/2048\n",
      "2s - loss: 0.4813 - val_loss: 0.5688\n",
      "Epoch 824/2048\n",
      "2s - loss: 0.4796 - val_loss: 0.5594\n",
      "Epoch 825/2048\n",
      "2s - loss: 0.4810 - val_loss: 0.5834\n",
      "Epoch 826/2048\n",
      "2s - loss: 0.4842 - val_loss: 0.5657\n",
      "Epoch 827/2048\n",
      "3s - loss: 0.4798 - val_loss: 0.5571\n",
      "Epoch 828/2048\n",
      "2s - loss: 0.4765 - val_loss: 0.5758\n",
      "Epoch 829/2048\n",
      "2s - loss: 0.4811 - val_loss: 0.5644\n",
      "Epoch 830/2048\n",
      "2s - loss: 0.4823 - val_loss: 0.5642\n",
      "Epoch 831/2048\n",
      "2s - loss: 0.4804 - val_loss: 0.5585\n",
      "Epoch 832/2048\n",
      "3s - loss: 0.4797 - val_loss: 0.5567\n",
      "Epoch 833/2048\n",
      "2s - loss: 0.4868 - val_loss: 0.5723\n",
      "Epoch 834/2048\n",
      "2s - loss: 0.4856 - val_loss: 0.5651\n",
      "Epoch 835/2048\n",
      "2s - loss: 0.4821 - val_loss: 0.5576\n",
      "Epoch 836/2048\n",
      "3s - loss: 0.4784 - val_loss: 0.5566\n",
      "Epoch 837/2048\n",
      "2s - loss: 0.4778 - val_loss: 0.5611\n",
      "Epoch 838/2048\n",
      "1s - loss: 0.4849 - val_loss: 0.5751\n",
      "Epoch 839/2048\n",
      "2s - loss: 0.4797 - val_loss: 0.5597\n",
      "Epoch 840/2048\n",
      "3s - loss: 0.4777 - val_loss: 0.5557\n",
      "Epoch 841/2048\n",
      "2s - loss: 0.4803 - val_loss: 0.5627\n",
      "Epoch 842/2048\n",
      "2s - loss: 0.4782 - val_loss: 0.5574\n",
      "Epoch 843/2048\n",
      "2s - loss: 0.4855 - val_loss: 0.5678\n",
      "Epoch 844/2048\n",
      "2s - loss: 0.4789 - val_loss: 0.5607\n",
      "Epoch 845/2048\n",
      "2s - loss: 0.4863 - val_loss: 0.5696\n",
      "Epoch 846/2048\n",
      "2s - loss: 0.4895 - val_loss: 0.5645\n",
      "Epoch 847/2048\n",
      "1s - loss: 0.4835 - val_loss: 0.5641\n",
      "Epoch 848/2048\n",
      "1s - loss: 0.4779 - val_loss: 0.5598\n",
      "Epoch 849/2048\n",
      "2s - loss: 0.4798 - val_loss: 0.5625\n",
      "Epoch 850/2048\n",
      "1s - loss: 0.4833 - val_loss: 0.5596\n",
      "Epoch 851/2048\n",
      "2s - loss: 0.4826 - val_loss: 0.5570\n",
      "Epoch 852/2048\n",
      "2s - loss: 0.4810 - val_loss: 0.5659\n",
      "Epoch 853/2048\n",
      "2s - loss: 0.4773 - val_loss: 0.5574\n",
      "Epoch 854/2048\n",
      "2s - loss: 0.4762 - val_loss: 0.5599\n",
      "Epoch 855/2048\n",
      "1s - loss: 0.4767 - val_loss: 0.5576\n",
      "Epoch 856/2048\n",
      "3s - loss: 0.4755 - val_loss: 0.5551\n",
      "Epoch 857/2048\n",
      "2s - loss: 0.4761 - val_loss: 0.5579\n",
      "Epoch 858/2048\n",
      "2s - loss: 0.4804 - val_loss: 0.5576\n",
      "Epoch 859/2048\n",
      "2s - loss: 0.4778 - val_loss: 0.5718\n",
      "Epoch 860/2048\n",
      "4s - loss: 0.4768 - val_loss: 0.5541\n",
      "Epoch 861/2048\n",
      "1s - loss: 0.4752 - val_loss: 0.5570\n",
      "Epoch 862/2048\n",
      "1s - loss: 0.4905 - val_loss: 0.5813\n",
      "Epoch 863/2048\n",
      "2s - loss: 0.4879 - val_loss: 0.5682\n",
      "Epoch 864/2048\n",
      "1s - loss: 0.4810 - val_loss: 0.5604\n",
      "Epoch 865/2048\n",
      "1s - loss: 0.4779 - val_loss: 0.5563\n",
      "Epoch 866/2048\n",
      "2s - loss: 0.4807 - val_loss: 0.5781\n",
      "Epoch 867/2048\n",
      "2s - loss: 0.4921 - val_loss: 0.5890\n",
      "Epoch 868/2048\n",
      "2s - loss: 0.4833 - val_loss: 0.5556\n",
      "Epoch 869/2048\n",
      "2s - loss: 0.4744 - val_loss: 0.5564\n",
      "Epoch 870/2048\n",
      "4s - loss: 0.4750 - val_loss: 0.5520\n",
      "Epoch 871/2048\n",
      "1s - loss: 0.4766 - val_loss: 0.5667\n",
      "Epoch 872/2048\n",
      "2s - loss: 0.4775 - val_loss: 0.5597\n",
      "Epoch 873/2048\n",
      "2s - loss: 0.4743 - val_loss: 0.5593\n",
      "Epoch 874/2048\n",
      "2s - loss: 0.4793 - val_loss: 0.5554\n",
      "Epoch 875/2048\n",
      "2s - loss: 0.4801 - val_loss: 0.5617\n",
      "Epoch 876/2048\n",
      "2s - loss: 0.4918 - val_loss: 0.5733\n",
      "Epoch 877/2048\n",
      "2s - loss: 0.4766 - val_loss: 0.5576\n",
      "Epoch 878/2048\n",
      "2s - loss: 0.4751 - val_loss: 0.5523\n",
      "Epoch 879/2048\n",
      "2s - loss: 0.4722 - val_loss: 0.5589\n",
      "Epoch 880/2048\n",
      "2s - loss: 0.4744 - val_loss: 0.5558\n",
      "Epoch 881/2048\n",
      "2s - loss: 0.4741 - val_loss: 0.5573\n",
      "Epoch 882/2048\n",
      "2s - loss: 0.4746 - val_loss: 0.5529\n",
      "Epoch 883/2048\n",
      "2s - loss: 0.4735 - val_loss: 0.5528\n",
      "Epoch 884/2048\n",
      "2s - loss: 0.4738 - val_loss: 0.5525\n",
      "Epoch 885/2048\n",
      "2s - loss: 0.4759 - val_loss: 0.5590\n",
      "Epoch 886/2048\n",
      "2s - loss: 0.4867 - val_loss: 0.5549\n",
      "Epoch 887/2048\n",
      "2s - loss: 0.4785 - val_loss: 0.5545\n",
      "Epoch 888/2048\n",
      "1s - loss: 0.4770 - val_loss: 0.5695\n",
      "Epoch 889/2048\n",
      "2s - loss: 0.4796 - val_loss: 0.5530\n",
      "Epoch 890/2048\n",
      "2s - loss: 0.4737 - val_loss: 0.5528\n",
      "Epoch 891/2048\n",
      "2s - loss: 0.4775 - val_loss: 0.5569\n",
      "Epoch 892/2048\n",
      "2s - loss: 0.4721 - val_loss: 0.5556\n",
      "Epoch 893/2048\n",
      "2s - loss: 0.4788 - val_loss: 0.5528\n",
      "Epoch 894/2048\n",
      "2s - loss: 0.4811 - val_loss: 0.5547\n",
      "Epoch 895/2048\n",
      "1s - loss: 0.4749 - val_loss: 0.5529\n",
      "Epoch 896/2048\n",
      "2s - loss: 0.4741 - val_loss: 0.5544\n",
      "Epoch 897/2048\n",
      "2s - loss: 0.4729 - val_loss: 0.5575\n",
      "Epoch 898/2048\n",
      "4s - loss: 0.4723 - val_loss: 0.5510\n",
      "Epoch 899/2048\n",
      "2s - loss: 0.4723 - val_loss: 0.5808\n",
      "Epoch 900/2048\n",
      "2s - loss: 0.4771 - val_loss: 0.5555\n",
      "Epoch 901/2048\n",
      "2s - loss: 0.4719 - val_loss: 0.5528\n",
      "Epoch 902/2048\n",
      "3s - loss: 0.4768 - val_loss: 0.5508\n",
      "Epoch 903/2048\n",
      "4s - loss: 0.4731 - val_loss: 0.5504\n",
      "Epoch 904/2048\n",
      "1s - loss: 0.4725 - val_loss: 0.5542\n",
      "Epoch 905/2048\n",
      "1s - loss: 0.4755 - val_loss: 0.5601\n",
      "Epoch 906/2048\n",
      "2s - loss: 0.4708 - val_loss: 0.5526\n",
      "Epoch 907/2048\n",
      "1s - loss: 0.4721 - val_loss: 0.5522\n",
      "Epoch 908/2048\n",
      "2s - loss: 0.4721 - val_loss: 0.5606\n",
      "Epoch 909/2048\n",
      "2s - loss: 0.4785 - val_loss: 0.5583\n",
      "Epoch 910/2048\n",
      "2s - loss: 0.4721 - val_loss: 0.5514\n",
      "Epoch 911/2048\n",
      "2s - loss: 0.4733 - val_loss: 0.5506\n",
      "Epoch 912/2048\n",
      "1s - loss: 0.4739 - val_loss: 0.5505\n",
      "Epoch 913/2048\n",
      "2s - loss: 0.4803 - val_loss: 0.5579\n",
      "Epoch 914/2048\n",
      "3s - loss: 0.4720 - val_loss: 0.5481\n",
      "Epoch 915/2048\n",
      "2s - loss: 0.4736 - val_loss: 0.5493\n",
      "Epoch 916/2048\n",
      "2s - loss: 0.4705 - val_loss: 0.5576\n",
      "Epoch 917/2048\n",
      "1s - loss: 0.4754 - val_loss: 0.5645\n",
      "Epoch 918/2048\n",
      "1s - loss: 0.4769 - val_loss: 0.5532\n",
      "Epoch 919/2048\n",
      "1s - loss: 0.4706 - val_loss: 0.5518\n",
      "Epoch 920/2048\n",
      "2s - loss: 0.4720 - val_loss: 0.5497\n",
      "Epoch 921/2048\n",
      "4s - loss: 0.4690 - val_loss: 0.5465\n",
      "Epoch 922/2048\n",
      "1s - loss: 0.4723 - val_loss: 0.5509\n",
      "Epoch 923/2048\n",
      "2s - loss: 0.4879 - val_loss: 0.5607\n",
      "Epoch 924/2048\n",
      "2s - loss: 0.4751 - val_loss: 0.5572\n",
      "Epoch 925/2048\n",
      "2s - loss: 0.4737 - val_loss: 0.5502\n",
      "Epoch 926/2048\n",
      "2s - loss: 0.4691 - val_loss: 0.5490\n",
      "Epoch 927/2048\n",
      "2s - loss: 0.4705 - val_loss: 0.5469\n",
      "Epoch 928/2048\n",
      "2s - loss: 0.4728 - val_loss: 0.5476\n",
      "Epoch 929/2048\n",
      "2s - loss: 0.4690 - val_loss: 0.5477\n",
      "Epoch 930/2048\n",
      "2s - loss: 0.4694 - val_loss: 0.5502\n",
      "Epoch 931/2048\n",
      "1s - loss: 0.4687 - val_loss: 0.5502\n",
      "Epoch 932/2048\n",
      "1s - loss: 0.4780 - val_loss: 0.5672\n",
      "Epoch 933/2048\n",
      "2s - loss: 0.4750 - val_loss: 0.5481\n",
      "Epoch 934/2048\n",
      "2s - loss: 0.4730 - val_loss: 0.5476\n",
      "Epoch 935/2048\n",
      "2s - loss: 0.4708 - val_loss: 0.5494\n",
      "Epoch 936/2048\n",
      "2s - loss: 0.4695 - val_loss: 0.5515\n",
      "Epoch 937/2048\n",
      "2s - loss: 0.4714 - val_loss: 0.5815\n",
      "Epoch 938/2048\n",
      "2s - loss: 0.4990 - val_loss: 0.5685\n",
      "Epoch 939/2048\n",
      "2s - loss: 0.4780 - val_loss: 0.5546\n",
      "Epoch 940/2048\n",
      "2s - loss: 0.4756 - val_loss: 0.5518\n",
      "Epoch 941/2048\n",
      "2s - loss: 0.4714 - val_loss: 0.5508\n",
      "Epoch 942/2048\n",
      "2s - loss: 0.4758 - val_loss: 0.5525\n",
      "Epoch 943/2048\n",
      "2s - loss: 0.4691 - val_loss: 0.5471\n",
      "Epoch 944/2048\n",
      "2s - loss: 0.4680 - val_loss: 0.5500\n",
      "Epoch 945/2048\n",
      "2s - loss: 0.4671 - val_loss: 0.5483\n",
      "Epoch 946/2048\n",
      "2s - loss: 0.4684 - val_loss: 0.5494\n",
      "Epoch 947/2048\n",
      "2s - loss: 0.4670 - val_loss: 0.5521\n",
      "Epoch 948/2048\n",
      "2s - loss: 0.4748 - val_loss: 0.5486\n",
      "Epoch 949/2048\n",
      "2s - loss: 0.4705 - val_loss: 0.5578\n",
      "Epoch 950/2048\n",
      "2s - loss: 0.4773 - val_loss: 0.5522\n",
      "Epoch 951/2048\n",
      "1s - loss: 0.4727 - val_loss: 0.5581\n",
      "Epoch 952/2048\n",
      "1s - loss: 0.4707 - val_loss: 0.5523\n",
      "Epoch 953/2048\n",
      "2s - loss: 0.4707 - val_loss: 0.5475\n",
      "Epoch 954/2048\n",
      "3s - loss: 0.4678 - val_loss: 0.5448\n",
      "Epoch 955/2048\n",
      "2s - loss: 0.4729 - val_loss: 0.5456\n",
      "Epoch 956/2048\n",
      "2s - loss: 0.4708 - val_loss: 0.5480\n",
      "Epoch 957/2048\n",
      "5s - loss: 0.4660 - val_loss: 0.5434\n",
      "Epoch 958/2048\n",
      "2s - loss: 0.4695 - val_loss: 0.5601\n",
      "Epoch 959/2048\n",
      "2s - loss: 0.4698 - val_loss: 0.5487\n",
      "Epoch 960/2048\n",
      "1s - loss: 0.4678 - val_loss: 0.5457\n",
      "Epoch 961/2048\n",
      "2s - loss: 0.4669 - val_loss: 0.5514\n",
      "Epoch 962/2048\n",
      "2s - loss: 0.4715 - val_loss: 0.5517\n",
      "Epoch 963/2048\n",
      "2s - loss: 0.4701 - val_loss: 0.5475\n",
      "Epoch 964/2048\n",
      "2s - loss: 0.4811 - val_loss: 0.5559\n",
      "Epoch 965/2048\n",
      "1s - loss: 0.4701 - val_loss: 0.5455\n",
      "Epoch 966/2048\n",
      "2s - loss: 0.4775 - val_loss: 0.5551\n",
      "Epoch 967/2048\n",
      "1s - loss: 0.4699 - val_loss: 0.5532\n",
      "Epoch 968/2048\n",
      "2s - loss: 0.4700 - val_loss: 0.5469\n",
      "Epoch 969/2048\n",
      "2s - loss: 0.4690 - val_loss: 0.5515\n",
      "Epoch 970/2048\n",
      "2s - loss: 0.4699 - val_loss: 0.5438\n",
      "Epoch 971/2048\n"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(X_train,\n",
    "                X_train_norm,\n",
    "                epochs=2048,\n",
    "                batch_size=256,\n",
    "                validation_split=0.25,\n",
    "                verbose=2,\n",
    "                callbacks=[early_stopper, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload saved model\n",
    "autoencoder = load_model(\"%s%s.h5\" % (model_directory, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "predictions = autoencoder.predict(X_test_norm)\n",
    "\n",
    "def get_error_df(X_test, predictions, mode=\"allmean\", n_highest = 100):\n",
    "    \n",
    "    if mode == \"allmean\":\n",
    "        return np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "    \n",
    "    elif mode == \"topn\":\n",
    "        temp = np.partition(-np.power(X_test - predictions, 2), n_highest)\n",
    "        result = -temp[:,:n_highest]\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    elif mode == \"perobj\":\n",
    "        mses = []\n",
    "        for l in legend:\n",
    "            mse = np.mean(\n",
    "                np.power(X_test[:,l[\"start\"]:l[\"end\"]] - predictions[:,l[\"start\"]:l[\"end\"]], 2),\n",
    "                axis=1)\n",
    "            mses.append(mse)\n",
    "     \n",
    "        return np.maximum.reduce(mses)\n",
    "    \n",
    "ae_error = get_error_df(X_test, predictions, mode=\"topn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_directory = \"/afs/cern.ch/user/t/tkrzyzek/Documents/Data-Certification/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(ae_error, open(model_directory + \"ae_error.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
