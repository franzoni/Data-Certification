{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n",
      "········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_default_dlopen_flags\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcusolver.so.8.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e33599d6a609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/opt/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# Running on GPU?\n",
    "import setGPU\n",
    "\n",
    "import getpass\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get permission to access EOS (Insert your NICE password)\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())\n",
    "\n",
    "# ## Load data, and labels\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "CUTOFF_ACC = 1000\n",
    "# CUTOFF_ACC = 10\n",
    "\n",
    "PDs  = {1: 'BTagCSV',\n",
    "        2: 'BTagMu',\n",
    "        3: 'Charmonium',\n",
    "        4: 'DisplacedJet',\n",
    "        5: 'DoubleEG',\n",
    "        6: 'DoubleMuon',\n",
    "        7: 'DoubleMuonLowMass',\n",
    "        8: 'FSQJets',\n",
    "        9: 'HighMultiplicityEOF',\n",
    "        10: 'HTMHT',\n",
    "        11: 'JetHT',\n",
    "        12: 'MET',\n",
    "        13: 'MinimumBias',\n",
    "        14: 'MuonEG',\n",
    "        15: 'MuOnia',\n",
    "        16: 'NoBPTX',\n",
    "        17: 'SingleElectron',\n",
    "        18: 'SingleMuon',\n",
    "        19: 'SinglePhoton',\n",
    "        20: 'Tau',\n",
    "        21: 'ZeroBias'}\n",
    "\n",
    "# Select PD\n",
    "nPD = 11\n",
    "\n",
    "data_directory = \"/eos/cms/store/user/fsiroky/consistentlumih5/\"\n",
    "golden_file = \"/afs/cern.ch/user/t/tkrzyzek/Documents/Data-Certification/labels/Golden2016.json\"\n",
    "label_file = \"/afs/cern.ch/user/t/tkrzyzek/Documents/Data-Certification/JetHT.json\"\n",
    "model_directory = \"/eos/user/t/tkrzyzek/autoencoder/active_learning/\"\n",
    "model_name = \"model\"\n",
    "model_base_name = \"model_base\"\n",
    "\n",
    "def get_file_list(directory, pds, npd, typeof, extension):\n",
    "    files = []\n",
    "    parts = [\"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    for p in parts:\n",
    "        files.append(\"%s%s_%s_%s%s\" % (directory, pds[npd], p, typeof, extension))\n",
    "    return files\n",
    "\n",
    "files = get_file_list(data_directory, PDs, nPD, \"background\", \".h5\")\n",
    "files = files + get_file_list(data_directory, PDs, nPD, \"signal\", \".h5\")\n",
    "\n",
    "# Load good and bad jets\n",
    "def get_data(files):\n",
    "    readout = np.empty([0,2813])\n",
    "    \n",
    "    for file in files:\n",
    "        jet = file.split(\"/\")[-1][:-3]\n",
    "        print(\"Reading: %s\" % jet)\n",
    "        try:\n",
    "            h5file = h5py.File(file, \"r\")\n",
    "            readout = np.concatenate((readout, h5file[jet][:]), axis=0)\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have %s. %s\" % (jet, error))\n",
    "            continue\n",
    "\n",
    "    return readout\n",
    "\n",
    "def get_error_df(X_test, predictions, mode=\"allmean\", n_highest = 100):\n",
    "    \n",
    "    if mode == \"allmean\":\n",
    "        return np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "    \n",
    "    elif mode == \"topn\":\n",
    "        temp = np.partition(-np.power(X_test - predictions, 2), n_highest)\n",
    "        result = -temp[:,:n_highest]\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    elif mode == \"perobj\":\n",
    "        mses = []\n",
    "        for l in legend:\n",
    "            mse = np.mean(\n",
    "                np.power(X_test[:,l[\"start\"]:l[\"end\"]] - predictions[:,l[\"start\"]:l[\"end\"]], 2),\n",
    "                axis=1)\n",
    "            mses.append(mse)\n",
    "     \n",
    "        return np.maximum.reduce(mses)\n",
    "    \n",
    "def find_optimal_cutoff(scores, y_true):\n",
    "    step_factor = CUTOFF_ACC\n",
    "    max_acc = 0\n",
    "    best_threshold = None\n",
    "    for threshold in tqdm(np.geomspace(min(scores), max(scores), step_factor)):\n",
    "        y_pred = [1 if e > threshold else 0 for e in scores]\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, max_acc\n",
    "\n",
    "data = pd.DataFrame(get_data(files))\n",
    "\n",
    "data[\"run\"] = data[2807].astype(int)\n",
    "data[\"lumi\"] = data[2808].astype(int)\n",
    "data[\"inst_lumi\"] = data[2809].astype(float)\n",
    "\n",
    "# Drop unnecessary meta data\n",
    "data.drop([2807, 2808, 2809, 2810, 2811, 2812], axis=1, inplace=True)\n",
    "\n",
    "# Sort by runID and then by lumiID\n",
    "data = data.sort_values([\"run\", \"lumi\"], ascending=[True,True])\n",
    "\n",
    "# Reset index\n",
    "data = data.reset_index(drop=True)  \n",
    "\n",
    "runIDs  = data[\"run\"].astype(int)\n",
    "lumiIDs = data[\"lumi\"].astype(int)\n",
    "luminosity = data[\"inst_lumi\"].astype(float)\n",
    "\n",
    "# Apply labels\n",
    "data_train = data\n",
    "\n",
    "output_json = json.load(open(label_file))\n",
    "golden_json = json.load(open(golden_file))\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid):\n",
    "    try:\n",
    "        for i in json_file[str(int(orig_runid))]:\n",
    "            if orig_lumid >= i[0] and orig_lumid <= i[1]:\n",
    "                return 0\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return 1\n",
    "\n",
    "def add_flags(sample):\n",
    "    return json_checker(output_json, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "def add_golden_flags(sample):\n",
    "    return json_checker(golden_json, sample[\"run\"], sample[\"lumi\"])\n",
    "\n",
    "data[\"label\"] = data.apply(add_flags, axis=1)\n",
    "data_train[\"label\"] = data.apply(add_golden_flags, axis=1)\n",
    "\n",
    "# Split the data\n",
    "PRE_TRAIN = 0.1\n",
    "\n",
    "split = round(PRE_TRAIN*len(data))\n",
    "\n",
    "runIDs = runIDs[split:]\n",
    "lumiIDs = lumiIDs[split:]\n",
    "luminosity = luminosity[split:]\n",
    "\n",
    "train = data_train.iloc[:split]\n",
    "X_train = train.iloc[:, 0:2806]\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "test = data.iloc[split:]\n",
    "X_test = test.iloc[:, 0:2806]\n",
    "y_test = np.asarray(test[\"label\"])\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "inliers = []\n",
    "outliers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 2806)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2000)              5614000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_25 (PReLU)           (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_26 (PReLU)           (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "p_re_lu_27 (PReLU)           (None, 500)               500       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "p_re_lu_28 (PReLU)           (None, 1000)              1000      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2000)              2002000   \n",
      "_________________________________________________________________\n",
      "p_re_lu_29 (PReLU)           (None, 2000)              2000      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2806)              5614806   \n",
      "_________________________________________________________________\n",
      "p_re_lu_30 (PReLU)           (None, 2806)              2806      \n",
      "=================================================================\n",
      "Total params: 16,242,612\n",
      "Trainable params: 16,242,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# DEFINE MODELS\n",
    "######################\n",
    "# Calculate class weights\n",
    "#classes = np.unique(y_train)\n",
    "#weights = class_weight.compute_class_weight('balanced', classes, y_train)\n",
    "#cw = {int(cls): weight for cls, weight in zip(classes, weights)}\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 7,\n",
    "    \"n_estimators\": 64, \n",
    "    \"random_state\": 42, \n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\" : 1,\n",
    "#    \"class_weight\": cw\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**params)\n",
    "\n",
    "# Train only on good\n",
    "X_train_good = X_train[y_train == 0]\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(input_layer)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(500, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, kernel_regularizer=keras.regularizers.l1_l2(10e-5))(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "decoder = Dense(input_dim)(encoded)\n",
    "decoder = prellll(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "adamm = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "adamm_base = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "early_stopper_base = EarlyStopping(monitor=\"val_loss\",\n",
    "                              patience=32,\n",
    "                              verbose=True,\n",
    "                              mode=\"auto\")\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, loss='mean_squared_error')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_name)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")\n",
    "checkpoint_callback_base = ModelCheckpoint((\"%s%s.h5\" % (model_directory, model_base_name)),\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      verbose=False,\n",
    "                                      save_best_only=True,\n",
    "                                      mode=\"min\")\n",
    "\n",
    "ae_scores = []\n",
    "ae_cutoffs = []\n",
    "ae_base_scores = []\n",
    "ae_base_cutoffs = []\n",
    "ms_scores = []\n",
    "rf_scores = []\n",
    "true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11859 samples, validate on 3954 samples\n",
      "Epoch 1/2048\n",
      "28s - loss: 74.6919 - val_loss: 63.3395\n",
      "Epoch 2/2048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2e4d98ea92e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                           callbacks=[early_stopper, checkpoint_callback]).history\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mae_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "# PRE-TRAIN\n",
    "######################\n",
    "history = autoencoder.fit(X_train_good,\n",
    "                          X_train_good,\n",
    "                          epochs=2048,\n",
    "                          batch_size=256,\n",
    "                          shuffle=True,\n",
    "                          validation_split=0.25,\n",
    "                          verbose=2,\n",
    "                          callbacks=[early_stopper, checkpoint_callback]).history\n",
    "\n",
    "ae_pred = autoencoder.predict(X_train)\n",
    "ae_score = get_error_df(X_train, ae_pred, mode=\"topn\")\n",
    "ae_cutoff, ae_acc = find_optimal_cutoff(ae_score, y_train)\n",
    "ae_base_cutoff = ae_cutoff\n",
    "\n",
    "# Mean square\n",
    "mean_square = np.mean(np.power(X_test, 2), axis=1)\n",
    "\n",
    "# Random forest\n",
    "rf.fit(X_train, y_train)\n",
    "X_rf = X_train\n",
    "y_rf = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1\n",
      "Number of inliers in the subset: 16228\n",
      "Number of anomalies in the subset: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 490.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 691.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14574.0614215\n",
      "FOLD: 2\n",
      "Number of inliers in the subset: 16360\n",
      "Number of anomalies in the subset: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 547.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 427.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2937.50682426\n",
      "FOLD: 3\n",
      "Number of inliers in the subset: 16294\n",
      "Number of anomalies in the subset: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 604.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 459.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46520.4975073\n",
      "FOLD: 4\n",
      "Number of inliers in the subset: 16251\n",
      "Number of anomalies in the subset: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 578.09it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 596.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4789.15249286\n",
      "FOLD: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 674.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 609.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of inliers in the subset: 16264\n",
      "Number of anomalies in the subset: 105\n",
      "1449922.47902\n",
      "FOLD: 6\n",
      "Number of inliers in the subset: 16257\n",
      "Number of anomalies in the subset: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 522.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 577.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19191.8633057\n",
      "FOLD: 7\n",
      "Number of inliers in the subset: 15762\n",
      "Number of anomalies in the subset: 607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 603.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 611.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49255.5390326\n",
      "FOLD: 8\n",
      "Number of inliers in the subset: 15756\n",
      "Number of anomalies in the subset: 612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 596.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 681.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376267.98876\n",
      "FOLD: 9\n",
      "Number of inliers in the subset: 16322\n",
      "Number of anomalies in the subset: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 451.76it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 614.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4337458891e+13\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# ACTIVE LEARNING LOOP\n",
    "######################\n",
    "FOLDS_NO = 9\n",
    "\n",
    "ranges = np.linspace(0, len(X_test), FOLDS_NO+1)\n",
    "\n",
    "autoencoder = load_model(\"%s%s.h5\" % (model_directory, model_name))\n",
    "autoencoder.save(\"%s%s.h5\" % (model_directory, model_base_name))\n",
    "\n",
    "for i in range(FOLDS_NO):\n",
    "    print(\"FOLD:\", i+1)\n",
    "    \n",
    "    # PREDICT\n",
    "    X_orig = X_test[int(ranges[i]):int(ranges[i+1])]\n",
    "    X = X_orig\n",
    "    y = y_test[int(ranges[i]):int(ranges[i+1])]\n",
    "\n",
    "    inliers.append(sum((y == 0)))\n",
    "    outliers.append(sum((y == 1)))\n",
    "    print(\"Number of inliers in the subset: %s\" % inliers[-1])\n",
    "    print(\"Number of anomalies in the subset: %s\" % outliers[-1])\n",
    "    \n",
    "    autoencoder = load_model(\"%s%s.h5\" % (model_directory, model_name))\n",
    "    autoencoder_base = load_model(\"%s%s.h5\" % (model_directory, model_base_name))\n",
    "    \n",
    "    ae_pred = autoencoder.predict(X)\n",
    "    ae_score = get_error_df(X, ae_pred, mode=\"topn\")\n",
    "    \n",
    "    mean_square = np.mean(np.power(X, 2), axis=1)    \n",
    "    \n",
    "    # CHOOSE MOST INFORMATIVE SAMPLES\n",
    "    RATIO = 0.2\n",
    "        \n",
    "    ae_cutoffs.append(ae_cutoff)\n",
    "    ae_base_cutoffs.append(ae_base_cutoff)\n",
    "    \n",
    "    ae_diff = pd.DataFrame({'diff': np.abs(ae_score - ae_cutoff)})\n",
    "    ae_diff = ae_diff.sort_values('diff').iloc[:round(RATIO*len(X))]\n",
    "    ae_idx = ae_diff.index.values\n",
    "    X_known_ae = X[ae_idx]\n",
    "    y_known_ae = y[ae_idx]\n",
    "    X_ae = X_known_ae[y_known_ae == 1]\n",
    "    \n",
    "    ae_base_idx = np.random.choice(len(X), round(RATIO*len(X)))\n",
    "    X_known_ae_base = X[ae_base_idx]\n",
    "    y_known_ae_base = y[ae_base_idx]\n",
    "    X_ae_base = X_known_ae_base[y_known_ae_base == 1]\n",
    "\n",
    "    # RETRAIN\n",
    "    autoencoder.fit(X_ae,\n",
    "                    X_ae,\n",
    "                    epochs=2048,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.25,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopper, checkpoint_callback])\n",
    "    \n",
    "    autoencoder_base.fit(X_ae_base,\n",
    "                    X_ae_base,\n",
    "                    epochs=2048,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.25,\n",
    "                    verbose=2,\n",
    "                    callbacks=[early_stopper_base, checkpoint_callback_base])\n",
    "\n",
    "    # PREDICT AGAIN\n",
    "    ae_pred = autoencoder.predict(X)\n",
    "    ae_score = get_error_df(X, ae_pred, mode=\"topn\")\n",
    "    ae_scores.append(ae_score)\n",
    "    \n",
    "    ae_pred = autoencoder.predict(X_known_ae)\n",
    "    ae_score = get_error_df(X_known_ae, ae_pred, mode=\"topn\")\n",
    "    ae_cutoff, ae_acc = find_optimal_cutoff(ae_score, y_known_ae)\n",
    "    \n",
    "    ae_pred_base = autoencoder_base.predict(X)\n",
    "    ae_base_score = get_error_df(X, ae_pred_base, mode=\"topn\")\n",
    "    ae_base_scores.append(ae_base_score)\n",
    "    \n",
    "    ae_pred_base = autoencoder.predict(X_known_ae_base)\n",
    "    ae_base_score = get_error_df(X_known_ae_base, ae_pred_base, mode=\"topn\")\n",
    "    ae_base_cutoff, ae_acc = find_optimal_cutoff(ae_base_score, y_known_ae_base)\n",
    "    print(ae_base_cutoff)\n",
    "    mean_square = np.mean(np.power(X, 2), axis=1)\n",
    "    ms_scores.append(mean_square)\n",
    "\n",
    "    true_labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(X_test, open(model_directory + \"x_test.p\", \"wb\"))\n",
    "pickle.dump(ae_scores, open(model_directory + \"ae_scores.p\", \"wb\"))\n",
    "pickle.dump(ae_cutoffs, open(model_directory + \"ae_cutoffs.p\", \"wb\"))\n",
    "pickle.dump(ae_base_scores, open(model_directory + \"ae_base_scores.p\", \"wb\"))\n",
    "pickle.dump(ae_base_cutoffs, open(model_directory + \"ae_base_cutoffs.p\", \"wb\"))\n",
    "pickle.dump(ms_scores, open(model_directory + \"ms_scores.p\", \"wb\"))\n",
    "# pickle.dump(rf_scores, open(model_directory + \"rf_scores.p\", \"wb\"))\n",
    "pickle.dump(true_labels, open(model_directory + \"true_labels.p\", \"wb\"))\n",
    "pickle.dump(inliers, open(model_directory + \"inliers.p\", \"wb\"))\n",
    "pickle.dump(outliers, open(model_directory + \"outliers.p\", \"wb\"))\n",
    "# pickle.dump(luminosity, open(model_directory + \"luminosity.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
